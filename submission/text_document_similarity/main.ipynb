{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the StringSimilarity Class\n",
    "\n",
    "## Overview\n",
    "\n",
    "The `StringSimilarity` class is a powerful tool designed for Natural Language Processing (NLP) applications, particularly focusing on computing the similarity between different text documents. Its primary function revolves around processing text, evaluating similarity scores, and managing a collection of text documents. \n",
    "\n",
    "## Class Structure and Functionality\n",
    "\n",
    "## Project Folder Structure\n",
    "\n",
    "The `StringSimilarity` class is part of a broader project structure, designed to streamline file management and ease of use. All essential files and folders are neatly organized within a main directory named \"text_document_similarity\". This structured approach ensures efficient access and processing of text data.\n",
    "\n",
    "### Main Directory: \"text_document_similarity\"\n",
    "\n",
    "- The `main.ipynb` Jupyter Notebook, which houses the `StringSimilarity` class, is located at the root of this main directory. This notebook serves as the central script for executing the class's functionality.\n",
    "- An example text file is also placed in the \"text_document_similarity\" directory. This file can be used for initial testing and demonstration purposes, providing a practical example of how the class processes and compares text.\n",
    "- The `Corpus` folder, a critical component of the project, is situated within the \"text_document_similarity\" directory. It contains a selection of text files, labeled `text1.txt`, `text2.txt`, and `text3.txt`. These files constitute the text corpus against which new documents are compared for similarity.\n",
    "- This consolidated folder structure not only ensures a tidy and logical organization of files but also simplifies the class's operation. Users can effortlessly navigate through the project, add new documents, and perform similarity analysis without the hassle of complex file management.\n",
    "\n",
    "By maintaining this clear and concise folder structure, the `StringSimilarity` class within the \"text_document_similarity\" directory stands as a user-friendly and efficient solution for text similarity assessment, enhancing the overall experience of users engaging with this NLP tool.\n",
    "\n",
    "\n",
    "### Initialization\n",
    "\n",
    "- The `__init__` method serves as the class constructor, setting up essential structures for document storage and processing. \n",
    "- Key structures initialized include:\n",
    "  - `document_pool`: A dictionary to hold processed documents.\n",
    "  - `vector_pool`: A repository for the vector representations of these documents.\n",
    "  - `dictionary`: A set to store unique words across all documents, crucial for vectorization and textual analysis.\n",
    "\n",
    "### Text Cleaning and Processing\n",
    "\n",
    "- Stopwords, common words with limited informational value, are listed for exclusion during text processing. \n",
    "- The class employs methods like `cleaning_text`, `string_to_list`, and `main_cleaning` for preparing text data. These methods collectively facilitate the removal of non-words, normalization of text, and exclusion of stopwords.\n",
    "\n",
    "### Document Management and Corpus Creation\n",
    "\n",
    "- Methods such as `add_documents`, `load_text`, and `create_corpus` enable efficient document management.\n",
    "- These functions allow adding individual texts or batches of texts, processing them, and updating the document pool and dictionary accordingly.\n",
    "\n",
    "### Vector Representation and Similarity Computation\n",
    "\n",
    "- The `create_vector` method translates documents into binary vectors based on the class's dictionary, capturing the presence or absence of words.\n",
    "- The class offers various similarity computation methods (`dot_product_normal`, `cosine_Similarity`, `Euclidean_distance`, `Jaccard_similarity`) to cater to different analytical needs.\n",
    "\n",
    "### User Interaction\n",
    "\n",
    "- The `user_interaction` method allows for versatile user engagement, enabling users to input a text string or specify a file for comparison.\n",
    "- Users can choose the similarity metric to apply, and the function returns a DataFrame showing the calculated similarity scores.\n",
    "\n",
    "## Application\n",
    "\n",
    "The `StringSimilarity` class is an embodiment of versatility and efficiency in text similarity analysis. Whether it's for academic research, content recommendation systems, or other NLP applications, this class provides a robust foundation for comparing text documents, making it a valuable asset in the field of computational linguistics and data analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#!pip install numpy\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import norm\n",
    "import re\n",
    "\n",
    "\n",
    "class StringSimilarity:\n",
    "\n",
    "    \"\"\"\n",
    "    A class for computing string similarity using various metrics.\n",
    "    This class provides functionality to clean and process text documents,\n",
    "    calculate similarity scores, and manage a collection of text documents.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the StringSimilarity class with empty structures for storing documents.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Dictionary to store the processed documents\n",
    "        self.document_pool = {}\n",
    "\n",
    "        # Dictionary to store vector representations of the documents\n",
    "        self.vector_pool = {}\n",
    "\n",
    "        # Set to store unique words across all documents\n",
    "        self.dictionary = set()\n",
    "\n",
    "        # list of stopwords for basic text filtering\n",
    "        self.stopwords = [\n",
    "\n",
    "            \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\n",
    "            \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\",\n",
    "            \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\n",
    "            \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\",\n",
    "            \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\",\n",
    "            \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\",\n",
    "            \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\",\n",
    "            \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\",\n",
    "            \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\",\n",
    "            \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\",\n",
    "            \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\",\n",
    "            \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"\n",
    "        ]\n",
    "\n",
    "    def add_documents(self, name, document):\n",
    "        \"\"\"\n",
    "        Manual adds a document to the document pool after processing it.\n",
    "\n",
    "        Args:\n",
    "            name (str): The name or identifier for the document.\n",
    "            document (str): The text of the document to be added.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If document or name is string\n",
    "            ValueError: If name or document is empty \n",
    "            ValueError: Processed document is empty. It might contain only stopwords or non-words\n",
    "            ValueError: If the processed document already exists in the document pool.\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(name, str) or not isinstance(document, str):\n",
    "\n",
    "            raise TypeError(\"Both name and document must be strings.\")\n",
    "\n",
    "        if not name:\n",
    "            raise ValueError(\"Document name is empty.\")\n",
    "\n",
    "        if not document:\n",
    "            raise ValueError(\"Document content is empty.\")\n",
    "\n",
    "        processed_document = self.main_cleaning(document)\n",
    "\n",
    "        if not processed_document:\n",
    "            raise ValueError(\n",
    "                \"Processed document is empty. It might contain only stopwords or non-words.\")\n",
    "\n",
    "        # Check if the document is not already in the pool\n",
    "        if processed_document not in list(self.document_pool.keys()):\n",
    "\n",
    "            self.document_pool[name] = processed_document\n",
    "\n",
    "            word_check = False\n",
    "            for i in processed_document:\n",
    "\n",
    "                if i not in self.dictionary:\n",
    "                    word_check = True\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            if word_check:\n",
    "\n",
    "                self.dictionary.update(set(processed_document))\n",
    "\n",
    "                # after a new document is added to pool, all vectors have to be updated because dictionary is longer.\n",
    "                self.update_vectorpool()\n",
    "            else:\n",
    "                self.vector_pool[name] = self.create_vector(processed_document)\n",
    "\n",
    "                print(\n",
    "                    \"Text has been added to the pool but no new vocabularies were added.\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"The text {processed_document} has already been added to pool\")\n",
    "\n",
    "    @staticmethod\n",
    "    def cleaning_text(text):\n",
    "        \"\"\"\n",
    "        Static method to clean a given text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be cleaned.\n",
    "\n",
    "        Returns:\n",
    "            str: The cleaned text.\n",
    "\n",
    "        Raises: \n",
    "            TypeError: If the input text is not a string.\n",
    "            ValueError: If Input text is empty or only contains whitespace\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(text, str):\n",
    "            raise TypeError(\"Input text must be a string.\")\n",
    "\n",
    "        if text.strip() == \"\":\n",
    "\n",
    "            raise ValueError(\n",
    "                \"Input text is empty or only contains whitespace.\")\n",
    "\n",
    "        text = text.strip()  # removes whitespaces in the beginning and end\n",
    "        # Removes hyphens or underscores that are surrounded by word characters.\n",
    "        text = re.sub(r'\\b[_-]+|(?<=\\w)[_-]+|[_-]+(?=\\w)', '', text)\n",
    "        # Replaces abbreviations or initials and optional trailing commas with a space.\n",
    "        text = re.sub(r'\\b(?:[a-zA-Z]\\.)+[a-zA-Z]?[,]*\\b', ' ', text)\n",
    "        text = re.sub(r\"\\W\", \" \", text)  # remove non words char\n",
    "        text = re.sub(r\"\\d\", \" \", text)  # remove digits char\n",
    "        text = re.sub(r\"[\\s]+\", \" \", text)  # remove extra white space\n",
    "        text = text.lower()  # lower char for matching\n",
    "\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def load_text(path):\n",
    "        \"\"\"\n",
    "        Static method to load text from a given file path.\n",
    "\n",
    "        Args:\n",
    "            path (str): The file path from which to load the text.\n",
    "\n",
    "        Returns:\n",
    "            list: The processed list of words from the file.\n",
    "\n",
    "        Raises: \n",
    "            ValueError: If input is not a string\n",
    "            FileNotFoundError: If the path can not be found within the operating system \n",
    "        \"\"\"\n",
    "        if not isinstance(path, str):\n",
    "\n",
    "            raise ValueError(\"The file path must be a string.\")\n",
    "\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"The file does not exist at the path: {path}\")\n",
    "\n",
    "        # add try ... except???\n",
    "        with open(path, 'r') as file:  # Automatically closes the file after reading\n",
    "\n",
    "            # file = StringSimilarity.string_to_list(file.read())\n",
    "\n",
    "            file = file.read()\n",
    "\n",
    "        return file\n",
    "\n",
    "    @staticmethod\n",
    "    def create_doc_list(curr_path):\n",
    "        \"\"\"\n",
    "        Static method to create a list of document names in the 'Corpus' directory.\n",
    "\n",
    "        Args:\n",
    "            curr_path (str): The current working directory path.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of filenames found in the 'Corpus' subdirectory.\n",
    "\n",
    "        Raises: \n",
    "            FileNotFoundError: If the path can not be found within the operating system \n",
    "        \"\"\"\n",
    "\n",
    "        # Construct the path to the 'Corpus' directory which contains .txt files\n",
    "        corpus_path = os.path.join(curr_path, 'Corpus')\n",
    "\n",
    "        if not os.path.exists(corpus_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"The file does not exist at the path: {corpus_path}\")\n",
    "\n",
    "        # List all files in the 'Corpus' directory\n",
    "        objects = os.listdir(corpus_path)\n",
    "\n",
    "        return objects\n",
    "\n",
    "    def create_corpus(self):\n",
    "        \"\"\"\n",
    "        Method to create a corpus by processing and adding text files from the 'Corpus' directory.\n",
    "        Updates the document pool with new documents and their processed content.\n",
    "\n",
    "        Returns:\n",
    "            str: A message indicating the outcome of the corpus creation\n",
    "\n",
    "        Raises: \n",
    "            Exception: If an unexpected error occurs during file processing.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the current working directory\n",
    "        path = os.getcwd()\n",
    "\n",
    "        # Retrieve the list of text files in the 'Corpus' directory\n",
    "\n",
    "        try:\n",
    "\n",
    "            text_files = StringSimilarity.create_doc_list(path)\n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "            raise Exception(f'Failed to create document list: {e}')\n",
    "\n",
    "        # create path to Corpus folder\n",
    "        corpus_path = os.path.join(path, 'Corpus')\n",
    "\n",
    "        # count number of documents\n",
    "        new_count = 0\n",
    "\n",
    "        for i in text_files:\n",
    "\n",
    "            # Process only text files and avoid duplicates\n",
    "            if i.endswith('.txt'):\n",
    "\n",
    "                # avoid duplicates in document pool\n",
    "                if i not in self.document_pool.keys():\n",
    "\n",
    "                    try:\n",
    "                        # Load and process the text file\n",
    "                        temp_text = StringSimilarity.load_text(\n",
    "                            os.path.join(corpus_path, i))\n",
    "\n",
    "                        temp_text = self.main_cleaning(temp_text)\n",
    "\n",
    "                        # Update the dictionary and document pool\n",
    "                        self.dictionary.update(set(temp_text))\n",
    "                        self.document_pool[i] = list(set(temp_text))\n",
    "                        new_count += 1\n",
    "                    except Exception as e:\n",
    "\n",
    "                        raise Exception(\n",
    "                            f'Failed to load document {i} because of {e}')\n",
    "\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # Update the vector pool with new vectors\n",
    "\n",
    "        self.update_vectorpool()\n",
    "\n",
    "        if new_count == 0:\n",
    "\n",
    "            return \"no new documents in folder\"\n",
    "        else:\n",
    "\n",
    "            return f\"There have been {str(new_count)} new documents added to the folder\"\n",
    "\n",
    "    @staticmethod\n",
    "    def string_to_list(string1):\n",
    "        \"\"\"\n",
    "        Static method to convert a cleaned string into a list of words.\n",
    "\n",
    "        Args:\n",
    "            string1 (str): The string to be converted.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of words from the string.\n",
    "\n",
    "        Raises: \n",
    "            TypeError: If string is not string\n",
    "            ValueError: If string is empty after cleaning \n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(string1, str):\n",
    "            raise TypeError(\"Input must be a string.\")\n",
    "\n",
    "        # Convert the cleaned string into a list of words\n",
    "        clean_text = StringSimilarity.cleaning_text(string1)\n",
    "\n",
    "        if not clean_text.strip():\n",
    "            raise ValueError(\n",
    "                \"Input string is empty or contains only whitespace after cleaning.\")\n",
    "\n",
    "        return clean_text.split()\n",
    "\n",
    "    def removing_stopwords(self, list_words):\n",
    "        \"\"\"\n",
    "        Method to remove stopwords from a list of words.\n",
    "\n",
    "        Args:\n",
    "            list_words (list): The list of words from which stopwords are to be removed.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of words with stopwords removed.\n",
    "\n",
    "        Raises: \n",
    "            TypError: If Type of Input is not a list of words \n",
    "            ValueError: If list from Input is empty\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        if not isinstance(list_words, list):\n",
    "            raise TypeError(\"Input must be a list of words.\")\n",
    "\n",
    "        if not list_words:\n",
    "            raise ValueError(\"Input list of words is empty.\")\n",
    "\n",
    "        # Filter out stopwords from the list of words\n",
    "        text_without_stop = [\n",
    "            word for word in list_words if word not in self.stopwords]\n",
    "\n",
    "        return text_without_stop\n",
    "\n",
    "    def main_cleaning(self, text):\n",
    "        \"\"\"\n",
    "        Method to perform cleaning of the text, converting it into a list of words and removing stopwords. Finally remove words which are shorter than 3\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be cleaned.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of cleaned words from the text.\n",
    "\n",
    "        Raises: \n",
    "            TypeError: If input is not a string \n",
    "            ValueError: If the input text is empty.\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(text, str):\n",
    "            raise TypeError(\"Input must be a string.\")\n",
    "\n",
    "        if text.strip() == \"\":\n",
    "            raise ValueError(\n",
    "                \"Input text is empty or only contains whitespace.\")\n",
    "\n",
    "        # Clean text, convert text to a list of words and remove stopwords\n",
    "        text_list = StringSimilarity.string_to_list(text)\n",
    "        text_list = self.removing_stopwords(text_list)\n",
    "\n",
    "        # Filter out words that are too short (e.g., less than 3 characters)\n",
    "        text_list = [word for word in text_list if len(word) > 2]\n",
    "\n",
    "        return text_list\n",
    "\n",
    "    def create_vector(self, word_list):\n",
    "        \"\"\"\n",
    "        Creates a binary vector representation for a given list of words.\n",
    "\n",
    "        Args:\n",
    "            word_list (list): A list of words to be converted into a vector.\n",
    "\n",
    "        Returns:\n",
    "            list: A binary vector where 1 represents the presence of a word from the word list in the dictionary.\n",
    "\n",
    "        Raises: \n",
    "            TypeError: If the input is not a list.\n",
    "            ValueError: If the input list is empty or the dictionary is not initialized.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(word_list, list):\n",
    "            raise TypeError(\"Input must be a list of words.\")\n",
    "\n",
    "        if not word_list:\n",
    "            raise ValueError(\"Input word list is empty.\")\n",
    "\n",
    "        if not self.dictionary:\n",
    "            raise ValueError(\n",
    "                \"Dictionary is not initialized. Add some documents first.\")\n",
    "\n",
    "        # Initialize a vector of zeros with the same length as the dictionary\n",
    "        vector = [0] * len(self.dictionary)\n",
    "\n",
    "        # Set elements to 1 in the vector for words present in the word list\n",
    "        for i, word in enumerate(self.dictionary):\n",
    "\n",
    "            if word in word_list:\n",
    "                vector[i] = 1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        return vector\n",
    "\n",
    "    def update_vectorpool(self):\n",
    "        \"\"\"\n",
    "        Updates the vector representations for all documents in the document pool.\n",
    "\n",
    "        Raises: \n",
    "            ValueError: If the document pool is empty.\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.document_pool:\n",
    "            raise ValueError(\n",
    "                \"Document pool is empty. Add some documents before updating the vector pool.\")\n",
    "\n",
    "        # Check if the dictionary is initialized\n",
    "        if not self.dictionary:\n",
    "            raise ValueError(\n",
    "                \"Dictionary is not initialized. Add some documents to create the dictionary.\")\n",
    "\n",
    "        try:\n",
    "            # Update vector for each document in the document pool\n",
    "            for i in self.document_pool.keys():\n",
    "                self.vector_pool[i] = self.create_vector(self.document_pool[i])\n",
    "\n",
    "            print(\"All vectors are updated\")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(\n",
    "                f\"An error occurred while updating the vector pool: {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def rank_vectors(dict1):\n",
    "        \"\"\"\n",
    "        Ranks vectors based on their values.\n",
    "\n",
    "        Args:\n",
    "            dict1 (dict): A dictionary of vectors to be ranked.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with vectors ranked in descending order of their values.\n",
    "\n",
    "        Raises:\n",
    "            TypeError: If the input is not a dictionary.\n",
    "            ValueError: If the input dictionary is empty.\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(dict1, dict):\n",
    "            raise TypeError(\"Input must be a dictionary.\")\n",
    "\n",
    "        if not dict1:\n",
    "            raise ValueError(\"Input dictionary is empty.\")\n",
    "\n",
    "        # Sort the dictionary in descending order based on values\n",
    "        return dict(sorted(dict1.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    def dot_product_normal(self, new_doc, new_vector):\n",
    "        \"\"\"\n",
    "        Calculates the dot product similarity between a new document and all documents in the document pool.\n",
    "\n",
    "        Args:\n",
    "            new_doc (str): The text of the new document.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of dot product similarity scores.\n",
    "\n",
    "        Raises:\n",
    "            Valueerror: If the type of new_doc is not list.\n",
    "            ValueError: If the new document list is empty.\n",
    "            ValueError: If the document pool is empty.\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(new_doc, list):\n",
    "            raise TypeError(\"The new document must be a list.\")\n",
    "\n",
    "        if len(new_doc) == 0:\n",
    "            raise ValueError(\"The list of words does not contains any words\")\n",
    "\n",
    "        if not self.document_pool:\n",
    "            raise ValueError(\n",
    "                \"Document pool is empty. Add some documents before calculating Euclidean distance.\")\n",
    "\n",
    "        final_dict = {}\n",
    "\n",
    "        # Calculate dot product with each document vector\n",
    "        for text in self.document_pool.keys():\n",
    "\n",
    "            final_dict[text] = np.dot(new_vector, self.vector_pool[text])\n",
    "\n",
    "        return StringSimilarity.rank_vectors(final_dict)\n",
    "\n",
    "    def cosine_Similarity(self, new_doc, new_vector):\n",
    "        \"\"\"\n",
    "        Calculates the cosine similarity between a new document and all documents in the document pool.\n",
    "\n",
    "        Args:\n",
    "            new_doc (str): The text of the new document.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of cosine similarity scores.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the new document is empty or only contains whitespace.\n",
    "            ValueError: If the document pool is empty.\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.document_pool:\n",
    "            raise ValueError(\n",
    "                \"Document pool is empty. Add some documents before calculating cosine similarity.\")\n",
    "\n",
    "        cosine_values = {}\n",
    "\n",
    "        # Calculate cosine similarity with each document vector\n",
    "        for i in self.document_pool.keys():\n",
    "\n",
    "            temp_vector = self.vector_pool[i]\n",
    "\n",
    "            if norm(new_vector)*norm(temp_vector) != 0:\n",
    "\n",
    "                cosine = np.dot(new_vector, temp_vector) / \\\n",
    "                    (norm(new_vector)*norm(temp_vector))\n",
    "\n",
    "                cosine_values[i] = cosine\n",
    "\n",
    "            else:\n",
    "                cosine_values[i] = 'no matches'\n",
    "\n",
    "        return StringSimilarity.rank_vectors(cosine_values)\n",
    "\n",
    "    def Euclidean_distance(self, new_doc, new_vector):\n",
    "        \"\"\"\n",
    "        Calculates the Euclidean distance between a new document and all documents in the document pool.\n",
    "\n",
    "        Args:\n",
    "            new_doc (str): The text of the new document.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of Euclidean distance scores.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If type of input new_doc is not a list. \n",
    "            ValueError: If the new document list is empty.\n",
    "            ValueError: If the document pool is empty.\n",
    "\n",
    "        \"\"\"\n",
    "        if not isinstance(new_doc, list):\n",
    "            raise TypeError(\"The new document must be a list.\")\n",
    "\n",
    "        if len(new_doc) == 0:\n",
    "            raise ValueError(\"The list of words does not contains any words\")\n",
    "\n",
    "        if not self.document_pool:\n",
    "            raise ValueError(\n",
    "                \"Document pool is empty. Add some documents before calculating Euclidean distance.\")\n",
    "\n",
    "        euclidean_values = {}\n",
    "\n",
    "        # Calculate Euclidean distance with each document vector\n",
    "        for i in self.document_pool.keys():\n",
    "\n",
    "            temp_vector = self.vector_pool[i]\n",
    "\n",
    "            dist = np.linalg.norm(np.array(temp_vector) - np.array(new_vector))\n",
    "            euclidean_values[i] = dist\n",
    "\n",
    "        return StringSimilarity.rank_vectors(euclidean_values)\n",
    "\n",
    "    def Jaccard_similarity(self, clean_words):\n",
    "        \"\"\"\n",
    "        Calculates the Jaccard similarity between a new document and all documents in the document pool.\n",
    "\n",
    "        Args:\n",
    "            new_doc (str): The text of the new document.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of Jaccard similarity scores.\n",
    "\n",
    "        Raises:\n",
    "            TypeError: If the new document is not a list.\n",
    "            ValueError: If the new document list \n",
    "            ValueError: If the document pool is empty.\n",
    "        \"\"\"\n",
    "        if not isinstance(clean_words, list):\n",
    "            raise TypeError(\"The new document must be a list.\")\n",
    "\n",
    "        if len(clean_words) == 0:\n",
    "            raise ValueError(\"The list of words does not contains any words\")\n",
    "\n",
    "        if not self.document_pool:\n",
    "            raise ValueError(\n",
    "                \"Document pool is empty. Add some documents before calculating Jaccard similarity.\")\n",
    "        jaccard_values = {}\n",
    "\n",
    "        clean_words = set(clean_words)\n",
    "\n",
    "        # Iterate over each document in the document pool\n",
    "        for name, words in self.document_pool.items():\n",
    "\n",
    "            set_old_words = set(words)\n",
    "\n",
    "            # Calculate the intersection and union\n",
    "            intersection = clean_words.intersection(set_old_words)\n",
    "            union = clean_words.union(set_old_words)\n",
    "\n",
    "            # Calculate Jaccard similarity and add to the dictionary\n",
    "            jaccard_sim = len(intersection) / len(union) if union else 0\n",
    "            jaccard_values[name] = jaccard_sim\n",
    "\n",
    "        return StringSimilarity.rank_vectors(jaccard_values)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_dataframe(dict1, dict2, dict3, dict4):\n",
    "        \"\"\"\n",
    "        Creates a DataFrame from four dictionaries of similarity scores by each method.\n",
    "\n",
    "        Args:\n",
    "            dict1, dict2, dict3 (dict): Dictionaries of similarity scores seperated by method.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A DataFrame with the similarity scores from the three dictionaries.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        df = pd.DataFrame([dict1, dict2, dict3, dict4])\n",
    "\n",
    "        df = df.T  # Transpose to have keys as rows\n",
    "\n",
    "        df.columns = [\"dot_product\", \"cosine\", \"Euclidean\", \"jaccard\"]\n",
    "\n",
    "        return df\n",
    "\n",
    "    def user_interaction(self, text_type, method=\"all\", export=\"No\"):\n",
    "        \"\"\"\n",
    "        Facilitates user interaction for comparing a new text with the document pool.\n",
    "\n",
    "        This function allows the user to input a text string or specify a file. It then performs text cleaning,\n",
    "        computes various similarity scores with the documents in the pool, and optionally exports the results to Excel.\n",
    "\n",
    "        Args:\n",
    "            text_type (str): Type of text input, either \"string\" or \"file\".\n",
    "            method (str, optional): The method to use for computing similarity scores. Defaults to \"all\".\n",
    "            export (str, optional): Option to export the results to an Excel file. Defaults to \"No\".\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A DataFrame showing the similarity scores of the new text with each document in the pool.\n",
    "\n",
    "        Raises:\n",
    "            TypeError: If text_type is not a string.\n",
    "            ValueError: If export is empty or not 'Yes'/'No'.\n",
    "            ValueError: If text_type is empty or not 'string'/'file'.\n",
    "            ValueError: If the input text is too long (more than 200 characters) or empty.\n",
    "            ValueError: If the specified file is not found in the directory.\n",
    "            Exception: For errors in loading the document or in similarity score calculations.\n",
    "            Exception: For errors encountered while creating the Excel file.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if text_type is a valid string\n",
    "        if not isinstance(text_type, str):\n",
    "            raise TypeError(\"The text_type must be a string.\")\n",
    "\n",
    "        # Check for empty string\n",
    "        if text_type.strip() == \"\":\n",
    "            raise ValueError(\n",
    "                \"text_type is empty or only contains whitespace. Please enter valid text as argument.\")\n",
    "\n",
    "        # Validate text_type value\n",
    "        if not export in [\"Yes\", \"No\"]:\n",
    "            raise ValueError(\"Argument needs to be either 'Yes' or 'No'.\")\n",
    "\n",
    "        if not text_type in [\"string\", \"file\"]:\n",
    "            raise ValueError(\"Argument needs to be either 'string' or 'file'.\")\n",
    "\n",
    "        # Handling 'string' input\n",
    "        if text_type == \"string\":\n",
    "            # Prompt the user to enter text\n",
    "            q1 = input('Enter a string under 500 characters')\n",
    "\n",
    "            # Check for length constraint\n",
    "            if len(q1) > 500:\n",
    "                raise ValueError(\"Your input text was too long!\")\n",
    "\n",
    "            # Check for empty input\n",
    "            if q1.strip() == \"\":\n",
    "                raise ValueError(\n",
    "                    \"Entered text is empty or only contains whitespace. Please enter valid text.\")\n",
    "\n",
    "        # Handling 'file' input\n",
    "        if text_type == \"file\":\n",
    "            # Prompt the user to enter file name\n",
    "            q1 = input(\n",
    "                'Enter the name of the document (needs to be in the same directory as the script)')\n",
    "\n",
    "            # Check for empty input\n",
    "            if q1.strip() == \"\":\n",
    "                raise ValueError(\n",
    "                    \"Entered text is empty or only contains whitespace. Please enter valid text.\")\n",
    "\n",
    "            # Check if file exists in directory\n",
    "            objects = os.listdir(os.getcwd())\n",
    "            if not q1 in objects:\n",
    "                raise ValueError(\n",
    "                    f\"The file {q1} is not in the directory of this Jupyter notebook.\")\n",
    "\n",
    "            # Attempt to load the text from the file\n",
    "            try:\n",
    "                q1 = self.load_text(q1)\n",
    "            except Exception as e:\n",
    "                raise Exception(f'Failed to load document {q1} because of {e}')\n",
    "\n",
    "        # Clean the text and create vector\n",
    "        clean_text = self.main_cleaning(q1)\n",
    "        new_vector = self.create_vector(clean_text)\n",
    "\n",
    "        # Compute and store similarity scores\n",
    "        try:\n",
    "            # Compute similarity scores\n",
    "            result1 = self.dot_product_normal(clean_text, new_vector)\n",
    "            result2 = self.cosine_Similarity(clean_text, new_vector)\n",
    "            result3 = self.Euclidean_distance(clean_text, new_vector)\n",
    "            result4 = self.Jaccard_similarity(clean_text)\n",
    "\n",
    "            # Generate DataFrame based on selected method\n",
    "            if method == \"all\":\n",
    "                final_df = StringSimilarity.create_dataframe(\n",
    "                    result1, result2, result3, result4)\n",
    "            elif method == \"dot\":\n",
    "                final_df = pd.DataFrame(list(result1.items()), columns=[\n",
    "                                        'Document', 'Dot Product Similarity'])\n",
    "            elif method == \"cosine\":\n",
    "                final_df = pd.DataFrame(list(result2.items()), columns=[\n",
    "                                        'Document', 'Cosine Similarity'])\n",
    "            elif method == \"euclidean\":\n",
    "                final_df = pd.DataFrame(list(result3.items()), columns=[\n",
    "                                        'Document', 'Euclidean Distance'])\n",
    "            elif method == \"jaccard\":\n",
    "                final_df = pd.DataFrame(list(result4.items()), columns=[\n",
    "                                        'Document', 'Jaccard Similarity'])\n",
    "        except Exception as e:\n",
    "            raise Exception(\n",
    "                f\"An error occurred while calculating similarity scores: {e}\")\n",
    "\n",
    "        # Handle export option\n",
    "        if export == \"No\":\n",
    "            return final_df\n",
    "\n",
    "        if export == \"Yes\":\n",
    "            try:\n",
    "\n",
    "                # export dataframe to results.xlsx\n",
    "                final_df.to_excel(\"results.xlsx\")\n",
    "                print(\"Excel has been created!\")\n",
    "                return final_df\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"An error occurred while creating Excel: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Class Initialization and Structure Overview\n",
    "In the first step, we initiate an instance of the `StringSimilarity` class. This action sets the foundation for our text similarity analysis, establishing the essential structures within the class. Upon initialization, the class creates various key components such as the `document_pool`, `vector_pool`, `dictionary`, and a predefined list of `stopwords`. These components are integral to the class's functionality.\n",
    "\n",
    "- The `document_pool` is a dictionary meant to store processed documents.\n",
    "- The `vector_pool` holds vector representations of these documents.\n",
    "- The `dictionary` is a set containing unique words found across all documents.\n",
    "- The `stopwords` list includes common words to be filtered out during text processing.\n",
    "\n",
    "After initializing the class, we print the initial state of each component. This display helps us understand the class's initial setup and confirms that the essential structures are in place, ready to be populated and utilized in subsequent steps of text similarity analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Class Initialization and Structure Overview\n",
    "similarity = StringSimilarity()\n",
    "\n",
    "# Display the initial state of the class\n",
    "print(\"Initial Document Pool:\", similarity.document_pool)\n",
    "print(\"Initial Vector Pool:\", similarity.vector_pool)\n",
    "print(\"Initial Dictionary:\", similarity.dictionary)\n",
    "print(\"Stopwords List:\", similarity.stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Adding Documents to the Corpus\n",
    "In this step, we demonstrate the process of adding documents to our corpus. This is a critical part of building the text analysis framework, as it populates the document pool with initial data for comparison. The `add_documents` method of the `StringSimilarity` class is utilized here. This method takes two arguments: a unique identifier for the document (like \"Doc1\") and the actual text of the document. After adding a sample document, we print out the updated list of document names present in the document pool. This allows us to verify that the document has been successfully added and is ready for further processing and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Adding Documents to the Corpus\n",
    "# Adding a small text manually\n",
    "similarity.add_documents(\"Doc1\", \"This is a sample document.\")\n",
    "print(\"Updated list of documents in Document Pool:\",\n",
    "      list(similarity.document_pool.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Loading Documents from Files\n",
    "In this step, we demonstrate the functionality of the `StringSimilarity` class for loading documents from external files. This capability is essential for processing and analyzing larger text documents that are stored as files, enabling the class to handle real-world data scenarios effectively.\n",
    "\n",
    "- We utilize the `load_text` method of the class to import text from an external file named \"example.txt\".\n",
    "- The `load_text` method is designed to read the content of the file and return it as a string, allowing for further processing.\n",
    "\n",
    "The result of this operation is printed to the console, displaying the loaded text. This step is crucial for verifying that the class can successfully access and retrieve text data from files, a common requirement in text analysis projects. It ensures that our `StringSimilarity` class is not just limited to handling manually inputted text but is also capable of working with pre-existing text documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Loading Documents from Files\n",
    "# Load text from an external file\n",
    "loaded_text = similarity.load_text(\"example.txt\")\n",
    "print(\"Loaded Text:\", loaded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Creating and Managing Corpus\n",
    "This step showcases the `StringSimilarity` class's ability to automate the creation and management of a text corpus. This is a vital feature for handling multiple documents simultaneously and efficiently.\n",
    "\n",
    "- The `create_corpus` method is called to process and add all documents located within the 'Corpus' folder. This folder contains various text files that make up our text corpus.\n",
    "- This method reads each file in the 'Corpus' folder, cleans and processes the text using the class's internal methods, and then adds the resulting data to the class's internal structures for document management.\n",
    "\n",
    "After executing this method, we print the current state of the class to confirm that the corpus has been successfully created and the internal structures (`document_pool`, `vector_pool`, `dictionary`, and `stopwords`) have been updated accordingly.\n",
    "\n",
    "- We also display the message returned by `create_corpus`, which indicates the outcome of the corpus creation process, such as the number of new documents added.\n",
    "\n",
    "This functionality exemplifies the class's robustness in handling multiple text files, automating the tedious process of manually adding each document. It's a demonstration of how the `StringSimilarity` class simplifies the management of a text corpus, making it a useful tool for large-scale text analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Creating and Managing Corpus\n",
    "# Process and add all documents from the 'Corpus' folder\n",
    "corpus_creation_message = similarity.create_corpus()\n",
    "\n",
    "# Display state of class after adding the documents\n",
    "print(\"\\n\")\n",
    "print(\"State has been updated after Corpus was created\")\n",
    "print(\"Initial Document Pool:\", similarity.document_pool)\n",
    "print(\"Initial Vector Pool:\", similarity.vector_pool)\n",
    "print(\"Initial Dictionary:\", similarity.dictionary)\n",
    "print(\"Stopwords List:\", similarity.stopwords)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(corpus_creation_message)\n",
    "print(\"Updated list of documents in Document Pool:\",\n",
    "      list(similarity.document_pool.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Text Preprocessing and Cleaning\n",
    "In this step, we demonstrate the crucial process of text preprocessing and cleaning, an essential part of any text analysis task. The `StringSimilarity` class provides a method `main_cleaning` that efficiently handles this process.\n",
    "\n",
    "- We start with a raw text string: `\"THIS is @n example string!!!stop-words, et 1, , 4 ,5 will be filtert__ out\"`. This string intentionally includes various elements like uppercase letters, special characters, numbers, and potential stop-words to illustrate the effectiveness of the cleaning process.\n",
    "- The `main_cleaning` method is applied to this raw text, which internally utilizes other class methods like `cleaning_text`, `string_to_list`, and `removing_stopwords`. These methods collectively perform various cleaning actions such as:\n",
    "    - Converting text to lowercase.\n",
    "    - Removing special characters and numbers.\n",
    "    - Stripping unnecessary whitespace.\n",
    "    - Filtering out stopwords.\n",
    "    - Excluding short words (less than 3 characters).\n",
    "\n",
    "- After cleaning, we display both the original and the cleaned text. The cleaned text should reflect the removal of unwanted elements and the standardization of the text format.\n",
    "\n",
    "This step is a vital demonstration of the `StringSimilarity` class's ability to preprocess text, which is a foundational step in preparing text data for further similarity analysis. It shows how the class streamlines the transformation of raw, unstructured text into a cleaner, more analyzable format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Text Preprocessing and Cleaning\n",
    "# Demonstrate text cleaning process\n",
    "raw_text = \"THIS is @n example string!!!stop-words, et 1, , 4 ,5 will be filtert__ out\"\n",
    "cleaned_text = similarity.main_cleaning(raw_text)\n",
    "print(\"Original Text:\", raw_text)\n",
    "print(\"Cleaned Text:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Vector Representation\n",
    "In this part of our demonstration, we focus on converting cleaned text into a binary vector using the `StringSimilarity` class. The vector representation is a pivotal aspect of text similarity analysis as it quantifies the text in a format that can be easily compared using various similarity metrics.\n",
    "\n",
    "- Using the `create_vector` method of the `StringSimilarity` class, we convert the previously cleaned text into a binary vector. This method works by mapping each word in the cleaned text against the class's word dictionary. \n",
    "- Each element of the vector corresponds to a unique word in the class’s dictionary. The binary value (0 or 1) for each element indicates the absence or presence of the corresponding word in the cleaned text.\n",
    "- The output is a binary vector, where '1' signifies the presence of a word from the cleaned text in the dictionary, and '0' indicates its absence.\n",
    "\n",
    "This step is essential in transforming textual data into a numerical format, a prerequisite for employing mathematical techniques for similarity calculation. By showcasing the `create_vector` function, we highlight how the `StringSimilarity` class facilitates the transition from textual to numerical analysis, setting the stage for the next steps in text similarity computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Vector Representation\n",
    "# Convert cleaned text to binary vector\n",
    "vector = similarity.create_vector(cleaned_text)\n",
    "print(\"Binary Vector Representation:\", vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Calculating Similarity Scores\n",
    "This step is the culmination of our journey through the `StringSimilarity` class, where we finally calculate similarity scores using various methods. The purpose here is to demonstrate how the class can be utilized to analyze the similarity of a new text document with the existing corpus.\n",
    "\n",
    "- First, we load an external text file named \"example.txt\" using the `load_text` method. This showcases the class's ability to process and prepare external documents for analysis.\n",
    "- We then clean this loaded text using the `main_cleaning` method to ensure it's in the right format for similarity analysis. This step is vital as it ensures consistency in data preparation.\n",
    "- Next, we convert the cleaned text into a binary vector using the `create_vector` method, which is essential for numerical comparison.\n",
    "- Now, we calculate similarity scores using four different methods: `dot_product_normal`, `cosine_Similarity`, `Euclidean_distance`, and `Jaccard_similarity`. Each of these methods provides a unique approach to assessing text similarity, highlighting the versatility of the `StringSimilarity` class.\n",
    "  - The Dot Product method emphasizes direct overlap in words.\n",
    "  - Cosine Similarity focuses on the orientation of the text in the vector space, making it suitable for texts of varying lengths.\n",
    "  - Euclidean Distance provides a 'straight-line' measure of similarity, intuitive in its approach.\n",
    "  - Jaccard Similarity uses the ratio of common words to total words, effective for binary comparisons.\n",
    "- Finally, we display the results for each similarity score, providing a comprehensive view of how the new document compares to the existing corpus across multiple dimensions of similarity.\n",
    "\n",
    "This step not only illustrates the practical application of the `StringSimilarity` class but also provides insights into how different similarity metrics can be used in real-world scenarios, such as text comparison, recommendation systems, and more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Calculating Similarity Scores\n",
    "# Using a new input text for similarity calculation\n",
    "loaded_text = similarity.load_text(\"example.txt\")\n",
    "new_text = similarity.main_cleaning(loaded_text)\n",
    "\n",
    "new_vector = similarity.create_vector(similarity.main_cleaning(loaded_text))\n",
    "\n",
    "# Calculate similarity scores\n",
    "dot_product_scores = similarity.dot_product_normal(new_text, new_vector)\n",
    "cosine_similarity_scores = similarity.cosine_Similarity(new_text, new_vector)\n",
    "euclidean_distance_scores = similarity.Euclidean_distance(new_text, new_vector)\n",
    "jaccard_similarity_scores = similarity.Jaccard_similarity(new_text)\n",
    "\n",
    "# Display results\n",
    "print(\"Dot Product Scores:\", dot_product_scores)\n",
    "print(\"Cosine Similarity Scores:\", cosine_similarity_scores)\n",
    "print(\"Euclidean Distance Scores:\", euclidean_distance_scores)\n",
    "print(\"Jaccard Similarity Scores:\", jaccard_similarity_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: User Interaction Demonstration\n",
    "In this final step, we demonstrate the user interaction capabilities of the `StringSimilarity` class. This step is crucial for showcasing how end users can easily utilize the class to analyze text similarity in practical scenarios.\n",
    "\n",
    "- Before we begin, we clean up our document pool by deleting \"Doc1,\" which was added in Step 2. This is done to maintain the purity of our corpus and ensure that our results are based on the original, unaltered corpus.\n",
    "- We then print the keys of the `document_pool` to confirm that \"Doc1\" has been successfully removed, ensuring our corpus is in its intended state.\n",
    "- Next, we simulate a user interaction scenario where a user inputs a string directly. We demonstrate this by using the `user_interaction` method with the \"string\" argument.\n",
    "- The `user_interaction` method is designed to be versatile, allowing users to either input a string directly or specify a file for text comparison. This flexibility makes the class highly accessible and user-friendly.\n",
    "- In this demonstration, we prompt the user to enter a string (in this case, \"Example user input text\"). The class then processes this input, calculates similarity scores using the different methods, and outputs the results.\n",
    "- This step exemplifies how the `StringSimilarity` class can be interactively used in real-world applications, such as content analysis, document retrieval, and text-based recommendation systems, where end-user interaction is a key component.\n",
    "\n",
    "Through this step, we not only show the interactive nature of the class but also its practical applicability in scenarios requiring direct user input for text similarity analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: User Interaction Demonstration\n",
    "\n",
    "# delete Doc1 created in step 2\n",
    "del similarity.document_pool['Doc1']\n",
    "\n",
    "print(similarity.document_pool.keys())\n",
    "\n",
    "# Example of user interaction with a string\n",
    "user_input = \"Example user input text\"\n",
    "similarity.user_interaction(\"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Analysis of Text Similarity Results Across Four Methods\n",
    "\n",
    "Our exploration into recommending books based on textual similarity employed four distinct methods: Dot Product, Cosine Similarity, Euclidean Distance, and Jaccard Similarity. Each method offers unique insights, and by analyzing their results, we can form a nuanced understanding of the textual relationships between our reference book and others in our corpus.\n",
    "\n",
    "### Dissecting the Results Across Methods\n",
    "\n",
    "#### Dot Product Similarity:\n",
    "- 'text2.txt' leads the pack with the highest score, indicating a substantial overlap in vocabulary with our reference book. This suggests thematic or stylistic parallels.\n",
    "- 'text1.txt' and 'text3.txt' follow, with moderate and lower scores respectively, pointing to lesser degrees of similarity.\n",
    "\n",
    "#### Cosine Similarity:\n",
    "- Reflecting on the orientation of texts in vector space, this method also places 'text2.txt' at the top. Its high score reinforces the idea of a strong thematic alignment with our reference book.\n",
    "- The scores for 'text1.txt' and 'text3.txt' are lower, suggesting varying levels of thematic divergence.\n",
    "\n",
    "#### Euclidean Distance:\n",
    "- In this metric, lower scores denote closer similarity. 'text2.txt' scores the lowest, aligning with previous findings and underscoring its closeness to the reference book.\n",
    "- 'text1.txt' and 'text3.txt' have higher scores, indicating more significant differences in word usage and content.\n",
    "\n",
    "#### Jaccard Similarity:\n",
    "- Consistent with earlier observations, 'text2.txt' scores highest, reflecting a high degree of shared unique words.\n",
    "- The scores for 'text1.txt' and 'text3.txt' are lower, indicating less overlap in specific word occurrences.\n",
    "\n",
    "### Interpreting the Results and Conclusions\n",
    "\n",
    "The consistency of 'text2.txt' scoring high across all methods strongly suggests it as the most similar book to our reference text. This uniformity across diverse metrics indicates that not only do these books share a significant number of words, but they also align closely in thematic and stylistic aspects.\n",
    "\n",
    "'text1.txt' presents as a moderately similar option, making it an appealing recommendation for readers seeking a balance of familiar and new content. 'text3.txt', while scoring lower across the board, could be recommended for readers looking for a more distinct but somewhat related reading experience.\n",
    "\n",
    "### Most Meaningful Measurement and Book Recommendation\n",
    "\n",
    "While each method provides valuable insights, Cosine Similarity stands out for its ability to normalize differences in text length and focus on the overall direction of the text in vector space. This makes it especially useful in capturing thematic similarities beyond mere word counts.\n",
    "\n",
    "Considering all the results, the most compelling book recommendation aligns with the consensus of our analysis: 'text2.txt'. Its consistent high scores across all metrics make it the closest match to our reference book, offering readers a familiar yet potentially enriching literary journey.\n",
    "\n",
    "In summary, this multi-faceted analysis not only highlights the intricate relationships between texts but also demonstrates the effectiveness of using a blend of similarity metrics for nuanced book recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example.txt\n",
    "similarity.user_interaction(\"file\", \"dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example.txt\n",
    "similarity.user_interaction(\"file\", \"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example.txt\n",
    "similarity.user_interaction(\"file\", \"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Jaccard Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text2.txt</td>\n",
       "      <td>0.253854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text1.txt</td>\n",
       "      <td>0.234624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text3.txt</td>\n",
       "      <td>0.163129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Document  Jaccard Similarity\n",
       "0  text2.txt            0.253854\n",
       "1  text1.txt            0.234624\n",
       "2  text3.txt            0.163129"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example.txt\n",
    "similarity.user_interaction(\"file\", \"jaccard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example.txt\n",
    "similarity.user_interaction(\"file\", export=\"Yes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
