{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from numpy.linalg import norm\n",
    "import re \n",
    "\n",
    "\n",
    "class StringSimilarity: \n",
    "    \n",
    "    \"\"\"\n",
    "    A class for computing string similarity using various metrics.\n",
    "    This class provides functionality to clean and process text documents,\n",
    "    calculate similarity scores, and manage a collection of text documents.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self): \n",
    "        \n",
    "        \"\"\"\n",
    "        Initializes the StringSimilarity class with empty structures for storing documents.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Dictionary to store the processed documents\n",
    "        self.document_pool = {}\n",
    "        \n",
    "        # Dictionary to store vector representations of the documents\n",
    "        self.vector_pool = {}\n",
    "        \n",
    "        # Set to store unique words across all documents\n",
    "        self.dictionary = set()\n",
    "        \n",
    "        \n",
    "        # list of stopwords for basic text filtering\n",
    "        self.stopwords = [\n",
    "            \n",
    "        \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \n",
    "        \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \n",
    "        \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \n",
    "        \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \n",
    "        \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \n",
    "        \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \n",
    "        \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \n",
    "        \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \n",
    "        \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \n",
    "        \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \n",
    "        \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \n",
    "        \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\" \n",
    "        ]\n",
    "     \n",
    "    \n",
    "    def add_documents(self,name, document): \n",
    "        \n",
    "        \"\"\"\n",
    "        Manual adds a document to the document pool after processing it.\n",
    "\n",
    "        Args:\n",
    "            name (str): The name or identifier for the document.\n",
    "            document (str): The text of the document to be added.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If document/string is empty\n",
    "            ValueError: If the processed document already exists in the document pool.\n",
    "        \"\"\"\n",
    "        \n",
    "        processed_document = self.main_cleaning(document)\n",
    "        \n",
    "        if document == \"\": \n",
    "            \n",
    "            raise ValueError(\"string is empty\")\n",
    "        \n",
    "        # Check if the document is not already in the pool\n",
    "        if processed_document not in list(self.document_pool.keys()): \n",
    "            \n",
    "            self.document_pool[name] = processed_document\n",
    "            \n",
    "            self.dictionary.update(set(processed_document))\n",
    "            \n",
    "            # after a new document is added to pool, all vectors have to be updated because dictionary is longer. \n",
    "            self.update_vectorpool() \n",
    "            \n",
    "        else: \n",
    "            raise ValueError(\"Text has already been added to pool\")\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def cleaning_text(text): \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Static method to clean a given text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be cleaned.\n",
    "\n",
    "        Returns:\n",
    "            str: The cleaned text.\n",
    "        \"\"\"\n",
    "        text = text.strip() # removes whitespaces in the beginning and end\n",
    "        text = re.sub(r'(?<=\\w)[_-]|[_-](?=\\w)', '', text) # Removes hyphens or underscores that are surrounded by word characters.\n",
    "        text = re.sub(r'\\b(?:[a-zA-Z]\\.)+[a-zA-Z]?[,]*\\b', ' ', text) # Replaces abbreviations or initials and optional trailing commas with a space.\n",
    "        text = re.sub(r\"\\W\", \" \", text)  #remove non words char\n",
    "        text = re.sub(r\"\\d\", \" \", text)  #remove digits char\n",
    "        text = re.sub(r\"[\\s]+\", \" \", text) # remove extra white space\n",
    "        text = text.lower() #lower char for matching\n",
    "        \n",
    "        return text \n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_text(path):\n",
    "        \n",
    "        \"\"\"\n",
    "        Static method to load text from a given file path.\n",
    "\n",
    "        Args:\n",
    "            path (str): The file path from which to load the text.\n",
    "\n",
    "        Returns:\n",
    "            list: The processed list of words from the file.\n",
    "        \"\"\"\n",
    "     \n",
    "        with open(path, 'r') as file: #Automatically closes the file after reading\n",
    "            \n",
    "            file = StringSimilarity.string_to_list(file.read())\n",
    "        \n",
    "        return file\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def create_doc_list(curr_path): \n",
    "        \n",
    "        \"\"\"\n",
    "        Static method to create a list of document names in the 'Corpus' directory.\n",
    "\n",
    "        Args:\n",
    "            curr_path (str): The current working directory path.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of filenames found in the 'Corpus' subdirectory.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Construct the path to the 'Corpus' directory which contains .txt files\n",
    "        corpus_path = os.path.join(curr_path, 'Corpus')\n",
    "\n",
    "        # List all files in the 'Corpus' directory\n",
    "        objects = os.listdir(corpus_path)\n",
    "        \n",
    "        return objects \n",
    "\n",
    "    def create_corpus(self): \n",
    "        \n",
    "        \"\"\"\n",
    "        Method to create a corpus by processing and adding text files from the 'Corpus' directory.\n",
    "        Updates the document pool with new documents and their processed content.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the current working directory\n",
    "        path = os.getcwd()\n",
    "        \n",
    "        # Retrieve the list of text files in the 'Corpus' directory\n",
    "        text_files = StringSimilarity.create_doc_list(path)\n",
    "        \n",
    "        # create path to Corpus folder \n",
    "        corpus_path = os.path.join(path, 'Corpus')\n",
    "        \n",
    "        # count number of documents\n",
    "        new_count = 0\n",
    "        \n",
    "        \n",
    "        for i in text_files: \n",
    "            \n",
    "            # Process only text files and avoid duplicates\n",
    "            if i.endswith('.txt'): \n",
    "                \n",
    "                # avoid duplicates in document pool\n",
    "                if i not in self.document_pool.keys(): \n",
    "                    \n",
    "                    # Load and process the text file\n",
    "                    temp_text = StringSimilarity.load_text(os.path.join(corpus_path, i))\n",
    "                    temp_text = self.removing_stopwords(temp_text)\n",
    "\n",
    "                    # Update the dictionary and document pool\n",
    "                    self.dictionary.update(set(temp_text))\n",
    "                    self.document_pool[i] = list(set(temp_text))\n",
    "                    new_count+= 1 \n",
    "                    \n",
    "                else: \n",
    "                    continue\n",
    "            else: \n",
    "                continue \n",
    "        \n",
    "        # Update the vector pool with new vectors\n",
    "        self.update_vectorpool()  \n",
    "        \n",
    "        if new_count == 0: \n",
    "            \n",
    "            return \"no new documents in folder\"\n",
    "        else: \n",
    "            \n",
    "            return f\"where have been {str(new_count)} new documents in the folder\"\n",
    "            \n",
    "        return \"Corpus created\"\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def string_to_list(string1): \n",
    "        \n",
    "        \"\"\"\n",
    "        Static method to convert a cleaned string into a list of words.\n",
    "\n",
    "        Args:\n",
    "            string1 (str): The string to be converted.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of words from the string.\n",
    "        \"\"\"\n",
    "        # Convert the cleaned string into a list of words\n",
    "        clean_text = StringSimilarity.cleaning_text(string1)\n",
    "\n",
    "        \n",
    "        return clean_text.split()\n",
    "\n",
    "\n",
    "    def removing_stopwords(self, list_words): \n",
    "        \n",
    "        \"\"\"\n",
    "        Method to remove stopwords from a list of words.\n",
    "\n",
    "        Args:\n",
    "            list_words (list): The list of words from which stopwords are to be removed.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of words with stopwords removed.\n",
    "        \"\"\"\n",
    "\n",
    "        # Filter out stopwords from the list of words\n",
    "        text_without_stop = [word for word in list_words if word not in self.stopwords]\n",
    "        \n",
    "        return text_without_stop\n",
    "    \n",
    "    \n",
    "    def main_cleaning(self, text): \n",
    "        \n",
    "        \"\"\"\n",
    "        Method to perform cleaning of the text, converting it into a list of words and removing stopwords.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be cleaned.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of cleaned words from the text.\n",
    "            \n",
    "        Raises: \n",
    "            ValueError: If the input text is empty.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        if not text:\n",
    "            raise ValueError(\"Input text is empty\")\n",
    "        \n",
    "        # Clean text, convert text to a list of words and remove stopwords\n",
    "        text_list = StringSimilarity.string_to_list(text)\n",
    "        text_list = self.removing_stopwords(text_list)\n",
    "        \n",
    "        return text_list      \n",
    "    \n",
    "   \n",
    "\n",
    "    def create_vector(self, word_list): \n",
    "        \n",
    "        \"\"\"\n",
    "        Creates a binary vector representation for a given list of words.\n",
    "\n",
    "        Args:\n",
    "            word_list (list): A list of words to be converted into a vector.\n",
    "\n",
    "        Returns:\n",
    "            list: A binary vector where 1 represents the presence of a word from the word list in the dictionary.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize a vector of zeros with the same length as the dictionary\n",
    "        vector = [0] * len(self.dictionary)\n",
    "        \n",
    "\n",
    "        # Set elements to 1 in the vector for words present in the word list\n",
    "        for i, word in enumerate(self.dictionary): \n",
    "            \n",
    "            if word in word_list: \n",
    "                vector[i] = 1\n",
    "            else: \n",
    "                continue \n",
    "            \n",
    "        return vector \n",
    "    \n",
    "    \n",
    "    def update_vectorpool(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Updates the vector representations for all documents in the document pool.\n",
    "        \n",
    "        \"\"\" \n",
    "        \n",
    "        # Update vector for each document in the document pool\n",
    "        for i in self.document_pool.keys(): \n",
    "            \n",
    "            self.vector_pool[i] = self.create_vector(self.document_pool[i])\n",
    "            \n",
    "        print(\"all vectors are updated\") \n",
    "\n",
    "    @staticmethod\n",
    "    def rank_vectors(dict1): \n",
    "        \n",
    "        \"\"\"\n",
    "        Ranks vectors based on their values.\n",
    "\n",
    "        Args:\n",
    "            dict1 (dict): A dictionary of vectors to be ranked.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with vectors ranked in descending order of their values.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        # Sort the dictionary in descending order based on values\n",
    "        return dict(sorted(dict1.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    def dot_product_normal(self, new_doc): \n",
    "        \n",
    "        \"\"\"\n",
    "        Calculates the dot product similarity between a new document and all documents in the document pool.\n",
    "\n",
    "        Args:\n",
    "            new_doc (str): The text of the new document.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of dot product similarity scores.\n",
    "        \"\"\"\n",
    "        \n",
    "        final_dict = {}\n",
    "        \n",
    "        \n",
    "        # cleans new text and create vector\n",
    "        clean_text = self.main_cleaning(new_doc)\n",
    "        new_vector = self.create_vector(clean_text)\n",
    "        \n",
    "        \n",
    "        # Calculate dot product with each document vector\n",
    "        for text in self.document_pool.keys(): \n",
    "\n",
    "            final_dict[text] = np.dot(new_vector, self.vector_pool[text])\n",
    "        \n",
    "        return StringSimilarity.rank_vectors(final_dict)\n",
    "    \n",
    "    \n",
    "\n",
    "    def cosine_Similarity(self, new_doc): \n",
    "        \n",
    "        \"\"\"\n",
    "        Calculates the cosine similarity between a new document and all documents in the document pool.\n",
    "\n",
    "        Args:\n",
    "            new_doc (str): The text of the new document.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of cosine similarity scores.\n",
    "        \"\"\"\n",
    "        \n",
    "        cosine_values = {}\n",
    "        \n",
    "        \n",
    "        # cleans new text and create vector\n",
    "        clean_text = self.main_cleaning(new_doc)\n",
    "        \n",
    "        new_vector = self.create_vector(clean_text)\n",
    "        \n",
    "        \n",
    "        # Calculate cosine similarity with each document vector\n",
    "        for i in self.document_pool.keys(): \n",
    "            \n",
    "            temp_vector = self.vector_pool[i]\n",
    "            \n",
    "            if norm(new_vector)*norm(temp_vector) != 0: \n",
    "                \n",
    "                cosine = np.dot(new_vector,temp_vector)/(norm(new_vector)*norm(temp_vector))\n",
    "                \n",
    "                cosine_values[i] = cosine\n",
    "                \n",
    "            else: \n",
    "                cosine_values[i] = 'no matches'\n",
    "            \n",
    "        return StringSimilarity.rank_vectors(cosine_values)\n",
    "    \n",
    "    \n",
    "    def Euclidean_distance(self, new_doc): \n",
    "        \n",
    "        \"\"\"\n",
    "        Calculates the Euclidean distance between a new document and all documents in the document pool.\n",
    "\n",
    "        Args:\n",
    "            new_doc (str): The text of the new document.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of Euclidean distance scores.\n",
    "        \"\"\"\n",
    "        \n",
    "        euclidean_values = {}\n",
    "        \n",
    "        # cleans new text and create vector\n",
    "        clean_text = self.main_cleaning(new_doc)\n",
    "        new_vector = self.create_vector(clean_text)\n",
    "        \n",
    "        \n",
    "        # Calculate Euclidean distance with each document vector\n",
    "        for i in self.document_pool.keys(): \n",
    "            \n",
    "            temp_vector = self.vector_pool[i]\n",
    "            \n",
    "            dist = np.linalg.norm(np.array(temp_vector) - np.array(new_vector))\n",
    "            euclidean_values[i] = dist \n",
    "            \n",
    "        return StringSimilarity.rank_vectors(euclidean_values)\n",
    "    \n",
    "    def Jaccard_similarity(self, new_doc): \n",
    "        \n",
    "        \"\"\"\n",
    "        Calculates the Jaccard similarity between a new document and all documents in the document pool.\n",
    "\n",
    "        Args:\n",
    "            new_doc (str): The text of the new document.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of Jaccard similarity scores.\n",
    "        \"\"\"\n",
    "        jaccard_values = {}\n",
    "        \n",
    "        # cleans new text and create set of words\n",
    "        clean_text = self.main_cleaning(new_doc)\n",
    "        set_new_words = set(clean_text)\n",
    "        \n",
    "        # Iterate over each document in the document pool\n",
    "        for name, words in self.document_pool.items(): \n",
    "            \n",
    "            set_old_words = set(words)\n",
    "            \n",
    "            # Calculate the intersection and union\n",
    "            intersection = set_new_words.intersection(set_old_words)\n",
    "            union = set_new_words.union(set_old_words)\n",
    "\n",
    "            # Calculate Jaccard similarity and add to the dictionary\n",
    "            jaccard_sim = len(intersection) / len(union) if union else 0\n",
    "            jaccard_values[name] = jaccard_sim\n",
    "        \n",
    "        return  jaccard_values \n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def create_dataframe(dict1, dict2, dict3, dict4): \n",
    "        \n",
    "        \"\"\"\n",
    "        Creates a DataFrame from four dictionaries of similarity scores by each method.\n",
    "\n",
    "        Args:\n",
    "            dict1, dict2, dict3 (dict): Dictionaries of similarity scores seperated by method.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A DataFrame with the similarity scores from the three dictionaries.\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.DataFrame([dict1,dict2, dict3, dict4])\n",
    "        \n",
    "        df = df.T # Transpose to have keys as rows\n",
    "    \n",
    "        df.columns = [\"dot_product\", \"cosine\", \"Euclidean\", \"jaccard\"]\n",
    "        \n",
    "        return df \n",
    "    \n",
    "    def user_interaction(self): \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Facilitates user interaction for comparing a new text with the document pool.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A DataFrame showing the similarity scores of the new text with each document in the pool.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Prompt the user to enter text\n",
    "        q1 = input('Please Enter the text you want to compare and press Enter')\n",
    "        \n",
    "        # Compute similarity scores\n",
    "        result1 = self.dot_product_normal(q1)\n",
    "        result2 = self.cosine_Similarity(q1)\n",
    "        result3 = self.Euclidean_distance(q1)\n",
    "        result4 = self.Jaccard_similarity(q1)\n",
    "\n",
    "        # Create and return a DataFrame with the results               \n",
    "        return StringSimilarity.create_dataframe(result1, result2, result3, result4)\n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all vectors are updated\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'where have been 2 new documents in the folder'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_pool3 = StringSimilarity()\n",
    "document_pool3.create_corpus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'access',\n",
       " 'account',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'address',\n",
       " 'adds',\n",
       " 'aims',\n",
       " 'already',\n",
       " 'also',\n",
       " 'amount',\n",
       " 'anonymous',\n",
       " 'another',\n",
       " 'app',\n",
       " 'applicant',\n",
       " 'application',\n",
       " 'applied',\n",
       " 'apply',\n",
       " 'approach',\n",
       " 'article',\n",
       " 'associated',\n",
       " 'attempting',\n",
       " 'authenticating',\n",
       " 'availability',\n",
       " 'became',\n",
       " 'beyond',\n",
       " 'birth',\n",
       " 'box',\n",
       " 'c',\n",
       " 'cannot',\n",
       " 'capture',\n",
       " 'case',\n",
       " 'civilian',\n",
       " 'clarity',\n",
       " 'clear',\n",
       " 'clearly',\n",
       " 'collected',\n",
       " 'communicate',\n",
       " 'compatible',\n",
       " 'complainant',\n",
       " 'complex',\n",
       " 'complexity',\n",
       " 'compliance',\n",
       " 'concerns',\n",
       " 'confirm',\n",
       " 'confirmed',\n",
       " 'confirming',\n",
       " 'confirms',\n",
       " 'consider',\n",
       " 'controller',\n",
       " 'copy',\n",
       " 'create',\n",
       " 'critical',\n",
       " 'data',\n",
       " 'datadriven',\n",
       " 'dataintensive',\n",
       " 'date',\n",
       " 'dating',\n",
       " 'demanding',\n",
       " 'details',\n",
       " 'digital',\n",
       " 'discussed',\n",
       " 'disproportionality',\n",
       " 'doubts',\n",
       " 'download',\n",
       " 'downloading',\n",
       " 'email',\n",
       " 'emphasizes',\n",
       " 'entered',\n",
       " 'essential',\n",
       " 'european',\n",
       " 'examining',\n",
       " 'exercise',\n",
       " 'extensive',\n",
       " 'form',\n",
       " 'fulfill',\n",
       " 'gdpr',\n",
       " 'goes',\n",
       " 'governmentissued',\n",
       " 'grindr',\n",
       " 'id',\n",
       " 'identify',\n",
       " 'identifying',\n",
       " 'identity',\n",
       " 'ids',\n",
       " 'impacts',\n",
       " 'includes',\n",
       " 'indicates',\n",
       " 'information',\n",
       " 'leads',\n",
       " 'less',\n",
       " 'limits',\n",
       " 'linked',\n",
       " 'long',\n",
       " 'looking',\n",
       " 'making',\n",
       " 'match',\n",
       " 'method',\n",
       " 'minimization',\n",
       " 'misalignment',\n",
       " 'must',\n",
       " 'natural',\n",
       " 'necessary',\n",
       " 'noyb',\n",
       " 'obligation',\n",
       " 'obtain',\n",
       " 'official',\n",
       " 'online',\n",
       " 'order',\n",
       " 'particularly',\n",
       " 'password',\n",
       " 'person',\n",
       " 'personal',\n",
       " 'photo',\n",
       " 'photos',\n",
       " 'platform',\n",
       " 'platforms',\n",
       " 'point',\n",
       " 'pointed',\n",
       " 'possible',\n",
       " 'prejudice',\n",
       " 'principle',\n",
       " 'privacy',\n",
       " 'process',\n",
       " 'processed',\n",
       " 'provided',\n",
       " 'providing',\n",
       " 'purposes',\n",
       " 'question',\n",
       " 'raises',\n",
       " 'reason',\n",
       " 'reasonable',\n",
       " 'regard',\n",
       " 'registration',\n",
       " 'related',\n",
       " 'relevant',\n",
       " 'represents',\n",
       " 'request',\n",
       " 'requester',\n",
       " 'required',\n",
       " 'requirement',\n",
       " 'requires',\n",
       " 'respect',\n",
       " 'right',\n",
       " 'rights',\n",
       " 'said',\n",
       " 'significant',\n",
       " 'simplicity',\n",
       " 'space',\n",
       " 'states',\n",
       " 'still',\n",
       " 'subject',\n",
       " 'submit',\n",
       " 'suitable',\n",
       " 'summary',\n",
       " 'taking',\n",
       " 'ticking',\n",
       " 'time',\n",
       " 'tool',\n",
       " 'transparently',\n",
       " 'understanding',\n",
       " 'union',\n",
       " 'use',\n",
       " 'user',\n",
       " 'users',\n",
       " 'verification',\n",
       " 'via',\n",
       " 'violates',\n",
       " 'way',\n",
       " 'whether',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_pool3.dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all vectors are updated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_string = \"If this is the case, they have the right to access their personal data held by the controller, thereby ensuring the transparency of the data processing. The controller should provide the data subject with a copy of all information about the data subject upon receipt of the request. The information should include the purpose of the data processing, the categories of personal data, the duration of storage, the recipients of the data, the rights to rectification, erasure or restriction of the data, the right to lodge a complaint, the source of the data if it was not collected from the data subject, and information on automated decision-making. (Trzaskowski and Gersvang Sørensen, 2022) Grindr has introduced two methods for applying Article 15. The first allows the user to download their data within the Grindr application and secondly the user can request the data with a form. The online form allows users to make personalized written requests about their data and request the deletion of their account and data\"\n",
    "document_pool3.add_documents(\"text1\", test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dot_product</th>\n",
       "      <th>cosine</th>\n",
       "      <th>Euclidean</th>\n",
       "      <th>jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>example1.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>no matches</td>\n",
       "      <td>10.099505</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text2.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>no matches</td>\n",
       "      <td>10.198039</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text1</th>\n",
       "      <td>0</td>\n",
       "      <td>no matches</td>\n",
       "      <td>7.681146</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             dot_product      cosine  Euclidean jaccard\n",
       "example1.txt           0  no matches  10.099505     0.0\n",
       "text2.txt              0  no matches  10.198039     0.0\n",
       "text1                  0  no matches   7.681146     0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_pool3.user_interaction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
