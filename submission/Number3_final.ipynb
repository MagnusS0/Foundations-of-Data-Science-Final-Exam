{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from numpy.linalg import norm\n",
    "import re \n",
    "\n",
    "\n",
    "class StringSimilarity: \n",
    "    \n",
    "    \"\"\"\n",
    "    A class for computing string similarity using various metrics.\n",
    "    This class provides functionality to clean and process text documents,\n",
    "    calculate similarity scores, and manage a collection of text documents.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self): \n",
    "        \n",
    "        \"\"\"\n",
    "        Initializes the StringSimilarity class with empty structures for storing documents.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Dictionary to store the processed documents\n",
    "        self.document_pool = {}\n",
    "        \n",
    "        # Dictionary to store vector representations of the documents\n",
    "        self.vector_pool = {}\n",
    "        \n",
    "        # Set to store unique words across all documents\n",
    "        self.dictionary = set()\n",
    "        \n",
    "        \n",
    "        # list of stopwords for basic text filtering\n",
    "        self.stopwords = [\n",
    "            \n",
    "        \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \n",
    "        \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \n",
    "        \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \n",
    "        \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \n",
    "        \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \n",
    "        \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \n",
    "        \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \n",
    "        \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \n",
    "        \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \n",
    "        \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \n",
    "        \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \n",
    "        \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\" \n",
    "        ]\n",
    "     \n",
    "    \n",
    "    def add_documents(self,name, document): \n",
    "        \n",
    "        \"\"\"\n",
    "        Manual adds a document to the document pool after processing it.\n",
    "\n",
    "        Args:\n",
    "            name (str): The name or identifier for the document.\n",
    "            document (str): The text of the document to be added.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If document or name is string\n",
    "            ValueError: If name or document is empty \n",
    "            ValueError: Processed document is empty. It might contain only stopwords or non-words\n",
    "            ValueError: If the processed document already exists in the document pool.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        if not isinstance(name, str) or not isinstance(document, str):\n",
    "            \n",
    "            \n",
    "            raise TypeError(\"Both name and document must be strings.\")\n",
    "        \n",
    "        if not name:\n",
    "            raise ValueError(\"Document name is empty.\")\n",
    "    \n",
    "        if not document:\n",
    "            raise ValueError(\"Document content is empty.\")\n",
    "        \n",
    "        \n",
    "        processed_document = self.main_cleaning(document)\n",
    "        \n",
    "        if not processed_document:\n",
    "            raise ValueError(\"Processed document is empty. It might contain only stopwords or non-words.\")\n",
    "        \n",
    "        \n",
    "        # Check if the document is not already in the pool\n",
    "        if processed_document not in list(self.document_pool.keys()): \n",
    "            \n",
    "            self.document_pool[name] = processed_document\n",
    "            \n",
    "            self.dictionary.update(set(processed_document))\n",
    "            \n",
    "            # after a new document is added to pool, all vectors have to be updated because dictionary is longer. \n",
    "            self.update_vectorpool() \n",
    "            \n",
    "        else: \n",
    "            raise ValueError(f\"The text {processed_document} has already been added to pool\")\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def cleaning_text(text): \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Static method to clean a given text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be cleaned.\n",
    "\n",
    "        Returns:\n",
    "            str: The cleaned text.\n",
    "            \n",
    "        Raises: \n",
    "            TypeError: If the input text is not a string.\n",
    "            ValueError: If Input text is empty or only contains whitespace\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        if not isinstance(text, str):\n",
    "            raise TypeError(\"Input text must be a string.\")\n",
    "        \n",
    "        if text.strip() == \"\":\n",
    "            \n",
    "            raise ValueError(\"Input text is empty or only contains whitespace.\")\n",
    "        \n",
    "        \n",
    "        text = text.strip() # removes whitespaces in the beginning and end\n",
    "        text = re.sub(r'(?<=\\w)[_-]|[_-](?=\\w)', '', text) # Removes hyphens or underscores that are surrounded by word characters.\n",
    "        text = re.sub(r'\\b(?:[a-zA-Z]\\.)+[a-zA-Z]?[,]*\\b', ' ', text) # Replaces abbreviations or initials and optional trailing commas with a space.\n",
    "        text = re.sub(r\"\\W\", \" \", text)  #remove non words char\n",
    "        text = re.sub(r\"\\d\", \" \", text)  #remove digits char\n",
    "        text = re.sub(r\"[\\s]+\", \" \", text) # remove extra white space\n",
    "        text = text.lower() #lower char for matching\n",
    "        \n",
    "        return text \n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_text(path):\n",
    "        \n",
    "        \"\"\"\n",
    "        Static method to load text from a given file path.\n",
    "\n",
    "        Args:\n",
    "            path (str): The file path from which to load the text.\n",
    "\n",
    "        Returns:\n",
    "            list: The processed list of words from the file.\n",
    "        \n",
    "        Raises: \n",
    "            ValueError: If input is not a string\n",
    "            FileNotFoundError: If the path can not be found within the operating system \n",
    "        \"\"\"\n",
    "        if not isinstance(path, str): \n",
    "            \n",
    "            raise ValueError(\"The file path must be a string.\")\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"The file does not exist at the path: {path}\")\n",
    "        \n",
    "        \n",
    "        # add try ... except???\n",
    "        with open(path, 'r') as file: #Automatically closes the file after reading\n",
    "            \n",
    "            file = StringSimilarity.string_to_list(file.read())\n",
    "        \n",
    "        return file\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def create_doc_list(curr_path): \n",
    "        \n",
    "        \"\"\"\n",
    "        Static method to create a list of document names in the 'Corpus' directory.\n",
    "\n",
    "        Args:\n",
    "            curr_path (str): The current working directory path.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of filenames found in the 'Corpus' subdirectory.\n",
    "            \n",
    "        Raises: \n",
    "            FileNotFoundError: If the path can not be found within the operating system \n",
    "        \"\"\"\n",
    "        \n",
    "        # Construct the path to the 'Corpus' directory which contains .txt files\n",
    "        corpus_path = os.path.join(curr_path, 'Corpus')\n",
    "        \n",
    "        if not os.path.exists(corpus_path):\n",
    "            raise FileNotFoundError(f\"The file does not exist at the path: {corpus_path}\")\n",
    "\n",
    "        # List all files in the 'Corpus' directory\n",
    "        objects = os.listdir(corpus_path)\n",
    "        \n",
    "        return objects \n",
    "\n",
    "    def create_corpus(self): \n",
    "        \n",
    "        \"\"\"\n",
    "        Method to create a corpus by processing and adding text files from the 'Corpus' directory.\n",
    "        Updates the document pool with new documents and their processed content.\n",
    "        \n",
    "        Returns:\n",
    "            str: A message indicating the outcome of the corpus creation\n",
    "        \n",
    "        Raises: \n",
    "            Exception: If an unexpected error occurs during file processing.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the current working directory\n",
    "        path = os.getcwd()\n",
    "        \n",
    "        # Retrieve the list of text files in the 'Corpus' directory\n",
    "        \n",
    "        try: \n",
    "            \n",
    "            text_files = StringSimilarity.create_doc_list(path)\n",
    "            \n",
    "        except Exception as e: \n",
    "            \n",
    "            raise Exception(f'Failed to create document list: {e}')\n",
    "        \n",
    "        # create path to Corpus folder \n",
    "        corpus_path = os.path.join(path, 'Corpus')\n",
    "        \n",
    "        # count number of documents\n",
    "        new_count = 0\n",
    "        \n",
    "        \n",
    "        for i in text_files: \n",
    "            \n",
    "            # Process only text files and avoid duplicates\n",
    "            if i.endswith('.txt'): \n",
    "                \n",
    "                # avoid duplicates in document pool\n",
    "                if i not in self.document_pool.keys(): \n",
    "                    \n",
    "                    \n",
    "                    try: \n",
    "                    # Load and process the text file\n",
    "                        temp_text = StringSimilarity.load_text(os.path.join(corpus_path, i))\n",
    "                        temp_text = self.removing_stopwords(temp_text)\n",
    "\n",
    "                        # Update the dictionary and document pool\n",
    "                        self.dictionary.update(set(temp_text))\n",
    "                        self.document_pool[i] = list(set(temp_text))\n",
    "                        new_count+= 1 \n",
    "                    except Exception as e: \n",
    "                        \n",
    "                        raise Exception(f'Failed to load document {i} because of {e}')\n",
    "                        \n",
    "                    \n",
    "                else: \n",
    "                    continue\n",
    "            else: \n",
    "                continue \n",
    "        \n",
    "        # Update the vector pool with new vectors\n",
    "        self.update_vectorpool()  \n",
    "        \n",
    "        if new_count == 0: \n",
    "            \n",
    "            return \"no new documents in folder\"\n",
    "        else: \n",
    "            \n",
    "            return f\"where have been {str(new_count)} new documents in the folder\"\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def string_to_list(string1): \n",
    "        \n",
    "        \"\"\"\n",
    "        Static method to convert a cleaned string into a list of words.\n",
    "\n",
    "        Args:\n",
    "            string1 (str): The string to be converted.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of words from the string.\n",
    "            \n",
    "        Raises: \n",
    "            TypeError: If string is not string\n",
    "            ValueError: If string is empty after cleaning \n",
    "        \"\"\"\n",
    "        \n",
    "        if not isinstance(string1, str):\n",
    "            raise TypeError(\"Input must be a string.\")\n",
    "        \n",
    "        \n",
    "        # Convert the cleaned string into a list of words\n",
    "        clean_text = StringSimilarity.cleaning_text(string1)\n",
    "\n",
    "        if not clean_text.strip():\n",
    "            raise ValueError(\"Input string is empty or contains only whitespace after cleaning.\")\n",
    "        \n",
    "        return clean_text.split()\n",
    "\n",
    "\n",
    "    def removing_stopwords(self, list_words): \n",
    "        \n",
    "        \"\"\"\n",
    "        Method to remove stopwords from a list of words.\n",
    "\n",
    "        Args:\n",
    "            list_words (list): The list of words from which stopwords are to be removed.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of words with stopwords removed.\n",
    "            \n",
    "        Raises: \n",
    "            TypError: If Type of Input is not a list of words \n",
    "            ValueError: If list from Input is empty\n",
    "\n",
    "            \n",
    "        \"\"\"\n",
    "        if not isinstance(list_words, list):\n",
    "            raise TypeError(\"Input must be a list of words.\")\n",
    "\n",
    "        if not list_words:\n",
    "            raise ValueError(\"Input list of words is empty.\")\n",
    "        \n",
    "        \n",
    "        # Filter out stopwords from the list of words\n",
    "        text_without_stop = [word for word in list_words if word not in self.stopwords]\n",
    "        \n",
    "        return text_without_stop\n",
    "    \n",
    "    \n",
    "    def main_cleaning(self, text): \n",
    "        \n",
    "        \"\"\"\n",
    "        Method to perform cleaning of the text, converting it into a list of words and removing stopwords.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be cleaned.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of cleaned words from the text.\n",
    "            \n",
    "        Raises: \n",
    "            TypeError: If input is not a string \n",
    "            ValueError: If the input text is empty.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not isinstance(text, str):\n",
    "            raise TypeError(\"Input must be a string.\")\n",
    "        \n",
    "        \n",
    "        if text.strip() == \"\":\n",
    "            raise ValueError(\"Input text is empty or only contains whitespace.\")\n",
    "        \n",
    "        # Clean text, convert text to a list of words and remove stopwords\n",
    "        text_list = StringSimilarity.string_to_list(text)\n",
    "        text_list = self.removing_stopwords(text_list)\n",
    "        \n",
    "        return text_list      \n",
    "    \n",
    "   \n",
    "\n",
    "    def create_vector(self, word_list): \n",
    "        \n",
    "        \"\"\"\n",
    "        Creates a binary vector representation for a given list of words.\n",
    "\n",
    "        Args:\n",
    "            word_list (list): A list of words to be converted into a vector.\n",
    "\n",
    "        Returns:\n",
    "            list: A binary vector where 1 represents the presence of a word from the word list in the dictionary.\n",
    "            \n",
    "        Raises: \n",
    "            TypeError: If the input is not a list.\n",
    "            ValueError: If the input list is empty or the dictionary is not initialized.\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        if not isinstance(word_list, list):\n",
    "            raise TypeError(\"Input must be a list of words.\")\n",
    "\n",
    "        if not word_list:\n",
    "            raise ValueError(\"Input word list is empty.\")\n",
    "\n",
    "        if not self.dictionary:\n",
    "            raise ValueError(\"Dictionary is not initialized. Add some documents first.\")\n",
    "        \n",
    "        # Initialize a vector of zeros with the same length as the dictionary\n",
    "        vector = [0] * len(self.dictionary)\n",
    "        \n",
    "\n",
    "        # Set elements to 1 in the vector for words present in the word list\n",
    "        for i, word in enumerate(self.dictionary): \n",
    "            \n",
    "            if word in word_list: \n",
    "                vector[i] = 1\n",
    "            else: \n",
    "                continue \n",
    "            \n",
    "        return vector \n",
    "    \n",
    "    \n",
    "    def update_vectorpool(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Updates the vector representations for all documents in the document pool.\n",
    "        \n",
    "        Raises: \n",
    "            ValueError: If the document pool is empty.\n",
    "        \"\"\" \n",
    "        \n",
    "        \n",
    "        if not self.document_pool:\n",
    "            raise ValueError(\"Document pool is empty. Add some documents before updating the vector pool.\")\n",
    "\n",
    "        # Check if the dictionary is initialized\n",
    "        if not self.dictionary:\n",
    "            raise ValueError(\"Dictionary is not initialized. Add some documents to create the dictionary.\")\n",
    "\n",
    "        try:\n",
    "            # Update vector for each document in the document pool\n",
    "            for i in self.document_pool.keys():\n",
    "                self.vector_pool[i] = self.create_vector(self.document_pool[i])\n",
    "\n",
    "            print(\"All vectors are updated\")\n",
    "            \n",
    "    \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"An error occurred while updating the vector pool: {e}\")\n",
    "        \n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def rank_vectors(dict1): \n",
    "        \n",
    "        \"\"\"\n",
    "        Ranks vectors based on their values.\n",
    "\n",
    "        Args:\n",
    "            dict1 (dict): A dictionary of vectors to be ranked.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with vectors ranked in descending order of their values.\n",
    "        \n",
    "        Raises:\n",
    "            TypeError: If the input is not a dictionary.\n",
    "            ValueError: If the input dictionary is empty.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        if not isinstance(dict1, dict):\n",
    "            raise TypeError(\"Input must be a dictionary.\")\n",
    "\n",
    "        if not dict1:\n",
    "            raise ValueError(\"Input dictionary is empty.\")  \n",
    "\n",
    "        \n",
    "        # Sort the dictionary in descending order based on values\n",
    "        return dict(sorted(dict1.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    def dot_product_normal(self, new_doc): \n",
    "        \n",
    "        \"\"\"\n",
    "        Calculates the dot product similarity between a new document and all documents in the document pool.\n",
    "\n",
    "        Args:\n",
    "            new_doc (str): The text of the new document.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of dot product similarity scores.\n",
    "        \n",
    "        Raises:\n",
    "            TypeError: If the new document is not a string.\n",
    "            ValueError: If the new document is empty or only contains whitespace.\n",
    "            ValueError: If the document pool is empty.\n",
    "        \"\"\"\n",
    "        if not isinstance(new_doc, str):\n",
    "            raise TypeError(\"The new document must be a string.\")\n",
    "        \n",
    "        if new_doc.strip() == \"\":\n",
    "            raise ValueError(\"The new document is empty or only contains whitespace.\")\n",
    "\n",
    "        if not self.document_pool:\n",
    "            raise ValueError(\"Document pool is empty. Add some documents before calculating dot product.\")\n",
    "        \n",
    "        final_dict = {}\n",
    "        \n",
    "        \n",
    "        # cleans new text and create vector\n",
    "        clean_text = self.main_cleaning(new_doc)\n",
    "        new_vector = self.create_vector(clean_text)\n",
    "        \n",
    "        \n",
    "        # Calculate dot product with each document vector\n",
    "        for text in self.document_pool.keys(): \n",
    "\n",
    "            final_dict[text] = np.dot(new_vector, self.vector_pool[text])\n",
    "        \n",
    "        return StringSimilarity.rank_vectors(final_dict)\n",
    "    \n",
    "    \n",
    "\n",
    "    def cosine_Similarity(self, new_doc): \n",
    "        \n",
    "        \"\"\"\n",
    "        Calculates the cosine similarity between a new document and all documents in the document pool.\n",
    "\n",
    "        Args:\n",
    "            new_doc (str): The text of the new document.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of cosine similarity scores.\n",
    "            \n",
    "        Raises:\n",
    "            TypeError: If the new document is not a string.\n",
    "            ValueError: If the new document is empty or only contains whitespace.\n",
    "            ValueError: If the document pool is empty.\n",
    "        \"\"\"\n",
    "        if not isinstance(new_doc, str):\n",
    "            raise TypeError(\"The new document must be a string.\")\n",
    "\n",
    "        if new_doc.strip() == \"\":\n",
    "            raise ValueError(\"The new document is empty or only contains whitespace.\")\n",
    "\n",
    "        if not self.document_pool:\n",
    "            raise ValueError(\"Document pool is empty. Add some documents before calculating cosine similarity.\")\n",
    "        \n",
    "        \n",
    "        cosine_values = {}\n",
    "        \n",
    "        \n",
    "        # cleans new text and create vector\n",
    "        clean_text = self.main_cleaning(new_doc)\n",
    "        \n",
    "        new_vector = self.create_vector(clean_text)\n",
    "        \n",
    "        \n",
    "        # Calculate cosine similarity with each document vector\n",
    "        for i in self.document_pool.keys(): \n",
    "            \n",
    "            temp_vector = self.vector_pool[i]\n",
    "            \n",
    "            if norm(new_vector)*norm(temp_vector) != 0: \n",
    "                \n",
    "                cosine = np.dot(new_vector,temp_vector)/(norm(new_vector)*norm(temp_vector))\n",
    "                \n",
    "                cosine_values[i] = cosine\n",
    "                \n",
    "            else: \n",
    "                cosine_values[i] = 'no matches'\n",
    "            \n",
    "        return StringSimilarity.rank_vectors(cosine_values)\n",
    "    \n",
    "    \n",
    "    def Euclidean_distance(self, new_doc): \n",
    "        \n",
    "        \"\"\"\n",
    "        Calculates the Euclidean distance between a new document and all documents in the document pool.\n",
    "\n",
    "        Args:\n",
    "            new_doc (str): The text of the new document.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of Euclidean distance scores.\n",
    "            \n",
    "        Raises:\n",
    "            TypeError: If the new document is not a string.\n",
    "            ValueError: If the new document is empty or only contains whitespace.\n",
    "            ValueError: If the document pool is empty.\n",
    "            \n",
    "        \"\"\"\n",
    "        if not isinstance(new_doc, str):\n",
    "            raise TypeError(\"The new document must be a string.\")\n",
    "\n",
    "        if new_doc.strip() == \"\":\n",
    "            raise ValueError(\"The new document is empty or only contains whitespace.\")\n",
    "\n",
    "        if not self.document_pool:\n",
    "            raise ValueError(\"Document pool is empty. Add some documents before calculating Euclidean distance.\")\n",
    "        \n",
    "        euclidean_values = {}\n",
    "        \n",
    "        # cleans new text and create vector\n",
    "        clean_text = self.main_cleaning(new_doc)\n",
    "        new_vector = self.create_vector(clean_text)\n",
    "        \n",
    "        \n",
    "        # Calculate Euclidean distance with each document vector\n",
    "        for i in self.document_pool.keys(): \n",
    "            \n",
    "            temp_vector = self.vector_pool[i]\n",
    "            \n",
    "            dist = np.linalg.norm(np.array(temp_vector) - np.array(new_vector))\n",
    "            euclidean_values[i] = dist \n",
    "            \n",
    "        return StringSimilarity.rank_vectors(euclidean_values)\n",
    "    \n",
    "    def Jaccard_similarity(self, new_doc): \n",
    "        \n",
    "        \"\"\"\n",
    "        Calculates the Jaccard similarity between a new document and all documents in the document pool.\n",
    "\n",
    "        Args:\n",
    "            new_doc (str): The text of the new document.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of Jaccard similarity scores.\n",
    "            \n",
    "        Raises:\n",
    "            TypeError: If the new document is not a string.\n",
    "            ValueError: If the new document is empty or only contains whitespace.\n",
    "            ValueError: If the document pool is empty.\n",
    "        \"\"\"\n",
    "        if not isinstance(new_doc, str):\n",
    "            raise TypeError(\"The new document must be a string.\")\n",
    "\n",
    "        if new_doc.strip() == \"\":\n",
    "            raise ValueError(\"The new document is empty or only contains whitespace.\")\n",
    "\n",
    "        if not self.document_pool:\n",
    "            raise ValueError(\"Document pool is empty. Add some documents before calculating Jaccard similarity.\")\n",
    "        jaccard_values = {}\n",
    "        \n",
    "        # cleans new text and create set of words\n",
    "        clean_text = self.main_cleaning(new_doc)\n",
    "        set_new_words = set(clean_text)\n",
    "        \n",
    "        # Iterate over each document in the document pool\n",
    "        for name, words in self.document_pool.items(): \n",
    "            \n",
    "            set_old_words = set(words)\n",
    "            \n",
    "            # Calculate the intersection and union\n",
    "            intersection = set_new_words.intersection(set_old_words)\n",
    "            union = set_new_words.union(set_old_words)\n",
    "\n",
    "            # Calculate Jaccard similarity and add to the dictionary\n",
    "            jaccard_sim = len(intersection) / len(union) if union else 0\n",
    "            jaccard_values[name] = jaccard_sim\n",
    "        \n",
    "        return  jaccard_values \n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def create_dataframe(dict1, dict2, dict3, dict4): \n",
    "        \n",
    "        \"\"\"\n",
    "        Creates a DataFrame from four dictionaries of similarity scores by each method.\n",
    "\n",
    "        Args:\n",
    "            dict1, dict2, dict3 (dict): Dictionaries of similarity scores seperated by method.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A DataFrame with the similarity scores from the three dictionaries.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        df = pd.DataFrame([dict1,dict2, dict3, dict4])\n",
    "        \n",
    "        df = df.T # Transpose to have keys as rows\n",
    "    \n",
    "        df.columns = [\"dot_product\", \"cosine\", \"Euclidean\", \"jaccard\"]\n",
    "        \n",
    "        return df \n",
    "    \n",
    "    def user_interaction(self): \n",
    "        \n",
    "        \"\"\"\n",
    "        Facilitates user interaction for comparing a new text with the document pool.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A DataFrame showing the similarity scores of the new text with each document in the pool.\n",
    "        Raises: \n",
    "            ValueError: If User input is empty\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Prompt the user to enter text\n",
    "        q1 = input('Please Enter the text you want to compare and press Enter')\n",
    "        \n",
    "        if q1.strip() == \"\":\n",
    "            raise ValueError(\"Entered text is empty or only contains whitespace. Please enter valid text.\")\n",
    "        \n",
    "        try: \n",
    "        # Compute similarity scores\n",
    "            result1 = self.dot_product_normal(q1)\n",
    "            result2 = self.cosine_Similarity(q1)\n",
    "            result3 = self.Euclidean_distance(q1)\n",
    "            result4 = self.Jaccard_similarity(q1)\n",
    "            \n",
    "            \n",
    "            # Create and return a DataFrame with the results               \n",
    "            return StringSimilarity.create_dataframe(result1, result2, result3, result4)\n",
    "        \n",
    "        except Exception as e:\n",
    "            \n",
    "            raise Exception(f\"An error occurred while calculating similarity scores: {e}\") \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All vectors are updated\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'where have been 4 new documents in the folder'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document_similarity = StringSimilarity()\n",
    "Document_similarity.create_corpus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text1.txt': ['lengths',\n",
       "  'today',\n",
       "  'shape',\n",
       "  'historical',\n",
       "  'ranging',\n",
       "  'computer',\n",
       "  'notion',\n",
       "  'system',\n",
       "  'foundational',\n",
       "  'realm',\n",
       "  'problemsolving',\n",
       "  'groundwork',\n",
       "  'rowan',\n",
       "  'cornerstone',\n",
       "  'determining',\n",
       "  'reasoning',\n",
       "  'like',\n",
       "  'euclidean',\n",
       "  'concepts',\n",
       "  'back',\n",
       "  'vectors',\n",
       "  'know',\n",
       "  'significant',\n",
       "  'electromagnetism',\n",
       "  'acting',\n",
       "  'century',\n",
       "  'reduction',\n",
       "  'work',\n",
       "  'processing',\n",
       "  'developing',\n",
       "  'mechanics',\n",
       "  'linear',\n",
       "  'array',\n",
       "  'capture',\n",
       "  'known',\n",
       "  'product',\n",
       "  'phenomena',\n",
       "  'meaningful',\n",
       "  'pivotal',\n",
       "  'projection',\n",
       "  'numbers',\n",
       "  'finds',\n",
       "  'remains',\n",
       "  'beyond',\n",
       "  'operation',\n",
       "  'origins',\n",
       "  'moving',\n",
       "  'intensity',\n",
       "  'multiplication',\n",
       "  'geometry',\n",
       "  'however',\n",
       "  'science',\n",
       "  'explored',\n",
       "  'scalar',\n",
       "  'dimensionality',\n",
       "  'made',\n",
       "  'rich',\n",
       "  'image',\n",
       "  'facilitates',\n",
       "  'geometric',\n",
       "  'mathematical',\n",
       "  'produce',\n",
       "  'roots',\n",
       "  'impact',\n",
       "  'widely',\n",
       "  'mathematicians',\n",
       "  'conclusion',\n",
       "  'involves',\n",
       "  'spaces',\n",
       "  'modern',\n",
       "  'particularly',\n",
       "  'quaternions',\n",
       "  'machines',\n",
       "  'moreover',\n",
       "  'optics',\n",
       "  'traced',\n",
       "  'one',\n",
       "  'learning',\n",
       "  'calculus',\n",
       "  'displacement',\n",
       "  'stands',\n",
       "  'disciplines',\n",
       "  'versatile',\n",
       "  'extended',\n",
       "  'euclid',\n",
       "  'multidimensional',\n",
       "  'fuels',\n",
       "  'th',\n",
       "  'torque',\n",
       "  'another',\n",
       "  'orthogonality',\n",
       "  'particle',\n",
       "  'comprehension',\n",
       "  'neural',\n",
       "  'indispensable',\n",
       "  'continues',\n",
       "  'innovation',\n",
       "  'line',\n",
       "  'domains',\n",
       "  'concept',\n",
       "  'graphics',\n",
       "  'calculation',\n",
       "  'refraction',\n",
       "  'classification',\n",
       "  'solving',\n",
       "  'angles',\n",
       "  'comprehending',\n",
       "  'also',\n",
       "  'machine',\n",
       "  'renowned',\n",
       "  'calculating',\n",
       "  'energy',\n",
       "  'utilized',\n",
       "  'enhancing',\n",
       "  'utility',\n",
       "  'object',\n",
       "  'use',\n",
       "  'onto',\n",
       "  'resulting',\n",
       "  'engineering',\n",
       "  'reflection',\n",
       "  'forces',\n",
       "  'fundamental',\n",
       "  'used',\n",
       "  'inner',\n",
       "  'plays',\n",
       "  'role',\n",
       "  'contemporary',\n",
       "  'modeling',\n",
       "  'compute',\n",
       "  'lighting',\n",
       "  'displaced',\n",
       "  'segments',\n",
       "  'world',\n",
       "  'early',\n",
       "  'value',\n",
       "  'magnetic',\n",
       "  'd',\n",
       "  'operations',\n",
       "  'networks',\n",
       "  'electric',\n",
       "  'interacts',\n",
       "  'space',\n",
       "  'calculates',\n",
       "  'william',\n",
       "  'aiding',\n",
       "  'support',\n",
       "  'contributions',\n",
       "  'describes',\n",
       "  'done',\n",
       "  'vision',\n",
       "  'extends',\n",
       "  'extensive',\n",
       "  'related',\n",
       "  'applications',\n",
       "  'classical',\n",
       "  'svms',\n",
       "  'charged',\n",
       "  'two',\n",
       "  'vector',\n",
       "  'relationships',\n",
       "  'four',\n",
       "  'force',\n",
       "  'laying',\n",
       "  'including',\n",
       "  'field',\n",
       "  'angle',\n",
       "  'defined',\n",
       "  'similarity',\n",
       "  'measure',\n",
       "  'history',\n",
       "  'ascertain',\n",
       "  'shading',\n",
       "  'complex',\n",
       "  'ancient',\n",
       "  'various',\n",
       "  'practical',\n",
       "  'dot',\n",
       "  'pertaining',\n",
       "  'rotational',\n",
       "  'charge',\n",
       "  'mathematics',\n",
       "  'across',\n",
       "  'hamilton',\n",
       "  'attributed',\n",
       "  'realism',\n",
       "  'pioneering',\n",
       "  'values',\n",
       "  'physics',\n",
       "  'formalization',\n",
       "  'hypercomplex',\n",
       "  'algorithms',\n",
       "  'objects',\n",
       "  'problems',\n",
       "  'algebra',\n",
       "  'effects',\n",
       "  'tasks',\n",
       "  'models',\n",
       "  'understanding',\n",
       "  'fields',\n",
       "  'dimensions',\n",
       "  'ability',\n",
       "  'alignment',\n",
       "  'instrumental',\n",
       "  'motion',\n",
       "  'sir',\n",
       "  'recognition',\n",
       "  'employed',\n",
       "  'yields',\n",
       "  'light',\n",
       "  'surface'],\n",
       " 'text2.txt': ['measuring',\n",
       "  'grounded',\n",
       "  'realm',\n",
       "  'closer',\n",
       "  'growth',\n",
       "  'natural',\n",
       "  'instances',\n",
       "  'educational',\n",
       "  'evident',\n",
       "  'relies',\n",
       "  'textual',\n",
       "  'like',\n",
       "  'model',\n",
       "  'engines',\n",
       "  'witness',\n",
       "  'categorization',\n",
       "  'vectors',\n",
       "  'significant',\n",
       "  'set',\n",
       "  'together',\n",
       "  'playing',\n",
       "  'utilize',\n",
       "  'processing',\n",
       "  'length',\n",
       "  'organization',\n",
       "  'era',\n",
       "  'known',\n",
       "  'insights',\n",
       "  'personalized',\n",
       "  'quantifying',\n",
       "  'pivotal',\n",
       "  'reveals',\n",
       "  'summarization',\n",
       "  'finds',\n",
       "  'considered',\n",
       "  'text',\n",
       "  'clustering',\n",
       "  'user',\n",
       "  'align',\n",
       "  'remains',\n",
       "  'symbolizing',\n",
       "  'search',\n",
       "  'range',\n",
       "  'plagiarism',\n",
       "  'versatility',\n",
       "  'mathematical',\n",
       "  'corpus',\n",
       "  'widely',\n",
       "  'facilitating',\n",
       "  'conclusion',\n",
       "  'mapped',\n",
       "  'represents',\n",
       "  'comparing',\n",
       "  'word',\n",
       "  'robust',\n",
       "  'reflects',\n",
       "  'one',\n",
       "  'signifies',\n",
       "  'stands',\n",
       "  'key',\n",
       "  'multidimensional',\n",
       "  'grasp',\n",
       "  'language',\n",
       "  'every',\n",
       "  'entire',\n",
       "  'detect',\n",
       "  'higher',\n",
       "  'continues',\n",
       "  'indispensable',\n",
       "  'similar',\n",
       "  'domains',\n",
       "  'collections',\n",
       "  'concept',\n",
       "  'systems',\n",
       "  'sentences',\n",
       "  'comprehending',\n",
       "  'employ',\n",
       "  'tool',\n",
       "  'cosine',\n",
       "  'representation',\n",
       "  'dimension',\n",
       "  'methods',\n",
       "  'documents',\n",
       "  'used',\n",
       "  'helps',\n",
       "  'role',\n",
       "  'represented',\n",
       "  'query',\n",
       "  'value',\n",
       "  'predefined',\n",
       "  'making',\n",
       "  'efficient',\n",
       "  'recommendations',\n",
       "  'evaluate',\n",
       "  'unique',\n",
       "  'recommendation',\n",
       "  'space',\n",
       "  'weight',\n",
       "  'given',\n",
       "  'principles',\n",
       "  'aiding',\n",
       "  'essential',\n",
       "  'correspond',\n",
       "  'related',\n",
       "  'applications',\n",
       "  'allows',\n",
       "  'two',\n",
       "  'vector',\n",
       "  'identify',\n",
       "  'wide',\n",
       "  'creators',\n",
       "  'closely',\n",
       "  'detection',\n",
       "  'magnitude',\n",
       "  'angle',\n",
       "  'similarity',\n",
       "  'us',\n",
       "  'data',\n",
       "  'calculate',\n",
       "  'items',\n",
       "  'various',\n",
       "  'relevant',\n",
       "  'technique',\n",
       "  'treats',\n",
       "  'mathematics',\n",
       "  'understand',\n",
       "  'exponential',\n",
       "  'across',\n",
       "  'pieces',\n",
       "  'retrieval',\n",
       "  'values',\n",
       "  'content',\n",
       "  'google',\n",
       "  'institutions',\n",
       "  'extracting',\n",
       "  'corresponding',\n",
       "  'similarities',\n",
       "  'document',\n",
       "  'information',\n",
       "  'fields',\n",
       "  'term',\n",
       "  'instrumental',\n",
       "  'large',\n",
       "  'applied',\n",
       "  'gauge',\n",
       "  'preferences',\n",
       "  'highdimensional',\n",
       "  'frequency',\n",
       "  'vocabulary',\n",
       "  'digital',\n",
       "  'valuable'],\n",
       " 'text3.txt': ['purposes',\n",
       "  'topics',\n",
       "  'realm',\n",
       "  'closer',\n",
       "  'implying',\n",
       "  'natural',\n",
       "  'exponentially',\n",
       "  'considerations',\n",
       "  'limitations',\n",
       "  'textual',\n",
       "  'euclidean',\n",
       "  'model',\n",
       "  'aids',\n",
       "  'like',\n",
       "  'engines',\n",
       "  'texts',\n",
       "  'vectors',\n",
       "  'different',\n",
       "  'news',\n",
       "  'significant',\n",
       "  'set',\n",
       "  'together',\n",
       "  'originally',\n",
       "  'utilize',\n",
       "  'processing',\n",
       "  'length',\n",
       "  'apply',\n",
       "  'corresponds',\n",
       "  'insights',\n",
       "  'represent',\n",
       "  'analysis',\n",
       "  'quantifying',\n",
       "  'meaningful',\n",
       "  'considered',\n",
       "  'vast',\n",
       "  'text',\n",
       "  'cluster',\n",
       "  'clustering',\n",
       "  'user',\n",
       "  'remains',\n",
       "  'larger',\n",
       "  'search',\n",
       "  'geometry',\n",
       "  'experience',\n",
       "  'even',\n",
       "  'assists',\n",
       "  'framework',\n",
       "  'based',\n",
       "  'topic',\n",
       "  'plagiarism',\n",
       "  'may',\n",
       "  'leading',\n",
       "  'geometric',\n",
       "  'mathematical',\n",
       "  'corpus',\n",
       "  'mathematically',\n",
       "  'dissimilarity',\n",
       "  'libraries',\n",
       "  'conclusion',\n",
       "  'spaces',\n",
       "  'techniques',\n",
       "  'word',\n",
       "  'one',\n",
       "  'quantify',\n",
       "  'improving',\n",
       "  'multidimensional',\n",
       "  'sensitivity',\n",
       "  'potentially',\n",
       "  'tend',\n",
       "  'representations',\n",
       "  'language',\n",
       "  'entire',\n",
       "  'alongside',\n",
       "  'higher',\n",
       "  'continues',\n",
       "  'similar',\n",
       "  'concept',\n",
       "  'shorter',\n",
       "  'serves',\n",
       "  'context',\n",
       "  'systems',\n",
       "  'relevance',\n",
       "  'certain',\n",
       "  'conceptually',\n",
       "  'tool',\n",
       "  'representation',\n",
       "  'cosine',\n",
       "  'subjects',\n",
       "  'enhancing',\n",
       "  'dimension',\n",
       "  'distances',\n",
       "  'documents',\n",
       "  'within',\n",
       "  'used',\n",
       "  'helps',\n",
       "  'modeling',\n",
       "  'represented',\n",
       "  'indicate',\n",
       "  'reference',\n",
       "  'dissimilar',\n",
       "  'query',\n",
       "  'retrieving',\n",
       "  'categorizing',\n",
       "  'unique',\n",
       "  'recommendation',\n",
       "  'mitigate',\n",
       "  'representing',\n",
       "  'space',\n",
       "  'weight',\n",
       "  'articles',\n",
       "  'potential',\n",
       "  'metrics',\n",
       "  'often',\n",
       "  'sense',\n",
       "  'grow',\n",
       "  'organizing',\n",
       "  'related',\n",
       "  'longer',\n",
       "  'applications',\n",
       "  'commonly',\n",
       "  'two',\n",
       "  'vector',\n",
       "  'identify',\n",
       "  'found',\n",
       "  'enables',\n",
       "  'help',\n",
       "  'closely',\n",
       "  'developed',\n",
       "  'including',\n",
       "  'relationships',\n",
       "  'submitted',\n",
       "  'derived',\n",
       "  'detection',\n",
       "  'provides',\n",
       "  'similarity',\n",
       "  'us',\n",
       "  'measure',\n",
       "  'data',\n",
       "  'reflect',\n",
       "  'age',\n",
       "  'practical',\n",
       "  'various',\n",
       "  'relevant',\n",
       "  'distance',\n",
       "  'amounts',\n",
       "  'pieces',\n",
       "  'retrieval',\n",
       "  'values',\n",
       "  'content',\n",
       "  'make',\n",
       "  'compare',\n",
       "  'extracting',\n",
       "  'document',\n",
       "  'understanding',\n",
       "  'information',\n",
       "  'fields',\n",
       "  'term',\n",
       "  'ability',\n",
       "  'manner',\n",
       "  'threshold',\n",
       "  'normalize',\n",
       "  'large',\n",
       "  'deals',\n",
       "  'applied',\n",
       "  'group',\n",
       "  'limitation',\n",
       "  'position',\n",
       "  'highdimensional',\n",
       "  'frequency',\n",
       "  'employed',\n",
       "  'digital',\n",
       "  'measures',\n",
       "  'valuable'],\n",
       " 'text4.txt': ['order',\n",
       "  'topics',\n",
       "  'measuring',\n",
       "  'realm',\n",
       "  'system',\n",
       "  'natural',\n",
       "  'treating',\n",
       "  'clear',\n",
       "  'sets',\n",
       "  'n',\n",
       "  'named',\n",
       "  'considerations',\n",
       "  'ease',\n",
       "  'limitations',\n",
       "  'textual',\n",
       "  'aids',\n",
       "  'engines',\n",
       "  'yet',\n",
       "  'categorization',\n",
       "  'set',\n",
       "  'way',\n",
       "  'interactions',\n",
       "  'processing',\n",
       "  'era',\n",
       "  'distinct',\n",
       "  'ranked',\n",
       "  'efficiently',\n",
       "  'insights',\n",
       "  'analysis',\n",
       "  'personalized',\n",
       "  'quantifying',\n",
       "  'finds',\n",
       "  'considered',\n",
       "  'text',\n",
       "  'clustering',\n",
       "  'user',\n",
       "  'remains',\n",
       "  'total',\n",
       "  'search',\n",
       "  'however',\n",
       "  'based',\n",
       "  'plagiarism',\n",
       "  'comparison',\n",
       "  'endures',\n",
       "  'degree',\n",
       "  'words',\n",
       "  'strengths',\n",
       "  'classifying',\n",
       "  'jaccard',\n",
       "  'conclusion',\n",
       "  'particularly',\n",
       "  'capturing',\n",
       "  'word',\n",
       "  'mathematician',\n",
       "  'one',\n",
       "  'versatile',\n",
       "  'useful',\n",
       "  'effectively',\n",
       "  'language',\n",
       "  'quantifies',\n",
       "  'higher',\n",
       "  'nuances',\n",
       "  'diverse',\n",
       "  'mining',\n",
       "  'abundant',\n",
       "  'similar',\n",
       "  'determine',\n",
       "  'domains',\n",
       "  'continues',\n",
       "  'classification',\n",
       "  'simplicity',\n",
       "  'context',\n",
       "  'systems',\n",
       "  'relevance',\n",
       "  'paul',\n",
       "  'calculating',\n",
       "  'tool',\n",
       "  'utilized',\n",
       "  'assess',\n",
       "  'behavior',\n",
       "  'span',\n",
       "  'documents',\n",
       "  'notable',\n",
       "  'helps',\n",
       "  'emerges',\n",
       "  'french',\n",
       "  'dissimilar',\n",
       "  'query',\n",
       "  'categorizing',\n",
       "  'effective',\n",
       "  'predefined',\n",
       "  'metric',\n",
       "  'making',\n",
       "  'recommendations',\n",
       "  'recommendation',\n",
       "  'compared',\n",
       "  'elements',\n",
       "  'ratio',\n",
       "  'offers',\n",
       "  'organizing',\n",
       "  'importance',\n",
       "  'applications',\n",
       "  'two',\n",
       "  'identify',\n",
       "  'allows',\n",
       "  'including',\n",
       "  'common',\n",
       "  'detection',\n",
       "  'provides',\n",
       "  'similarity',\n",
       "  'data',\n",
       "  'measure',\n",
       "  'recommending',\n",
       "  'items',\n",
       "  'categories',\n",
       "  'various',\n",
       "  'evolve',\n",
       "  'relevant',\n",
       "  'treats',\n",
       "  'size',\n",
       "  'comes',\n",
       "  'liked',\n",
       "  'retrieval',\n",
       "  'content',\n",
       "  'ngrams',\n",
       "  'make',\n",
       "  'overlap',\n",
       "  'likeness',\n",
       "  'sequences',\n",
       "  'extracting',\n",
       "  'extent',\n",
       "  'accordingly',\n",
       "  'number',\n",
       "  'tastes',\n",
       "  'tasks',\n",
       "  'document',\n",
       "  'understanding',\n",
       "  'information',\n",
       "  'landscape',\n",
       "  'users',\n",
       "  'additionally',\n",
       "  'terms',\n",
       "  'duplicate',\n",
       "  'applied',\n",
       "  'gauge',\n",
       "  'preferences',\n",
       "  'sensitive',\n",
       "  'gauging',\n",
       "  'employed',\n",
       "  'shared',\n",
       "  'straightforward',\n",
       "  'frequency',\n",
       "  'digital',\n",
       "  'interpretation',\n",
       "  'valuable']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document_similarity.document_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all vectors are updated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_string = \"If this is the case, they have the right to access their personal data held by the controller, thereby ensuring the transparency of the data processing. The controller should provide the data subject with a copy of all information about the data subject upon receipt of the request. The information should include the purpose of the data processing, the categories of personal data, the duration of storage, the recipients of the data, the rights to rectification, erasure or restriction of the data, the right to lodge a complaint, the source of the data if it was not collected from the data subject, and information on automated decision-making. (Trzaskowski and Gersvang Srensen, 2022) Grindr has introduced two methods for applying Article 15. The first allows the user to download their data within the Grindr application and secondly the user can request the data with a form. The online form allows users to make personalized written requests about their data and request the deletion of their account and data\"\n",
    "Document_similarity.add_documents(\"text1\", test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dot_product</th>\n",
       "      <th>cosine</th>\n",
       "      <th>Euclidean</th>\n",
       "      <th>jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text3.txt</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.277617</td>\n",
       "      <td>12.767145</td>\n",
       "      <td>0.090498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text2.txt</th>\n",
       "      <td>17.0</td>\n",
       "      <td>0.252581</td>\n",
       "      <td>12.124356</td>\n",
       "      <td>0.084158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text1.txt</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.150827</td>\n",
       "      <td>14.730920</td>\n",
       "      <td>0.044944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text4.txt</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.161312</td>\n",
       "      <td>12.767145</td>\n",
       "      <td>0.051887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dot_product    cosine  Euclidean   jaccard\n",
       "text3.txt         20.0  0.277617  12.767145  0.090498\n",
       "text2.txt         17.0  0.252581  12.124356  0.084158\n",
       "text1.txt         12.0  0.150827  14.730920  0.044944\n",
       "text4.txt         11.0  0.161312  12.767145  0.051887"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document_similarity.user_interaction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
